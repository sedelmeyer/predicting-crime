####################################################################
# DEFINE FUNCTIONS THAT CONTRIBUTE TO GENERATING A FITTED MODEL DICT 
####################################################################

# imports required
# requires sklearn>=v0.22 for multi-class auc
import os
import joblib
import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, auc, roc_auc_score


def summarize_predictions(predictions_train, predictions_test):
    """
    Generates a summary table of the training and test predictions, showing count
    and proportion by class
    
    predictions_train: np.array, the predicted classes generated on the training data
    predictions_test:np.array, the predicted classes generated on the test data
    
    returns: tuple of 2 pd.DataFrame objects, one summarizing the training predictions
             the other summarizing the test predictions
    """
    frame_name_list = ['pred_counts_train', 'pred_counts_test']
    
    for predictions, name in zip(
        [predictions_train, predictions_test],
        frame_name_list
    ):
        locals()[name] = np.array(np.unique(predictions, return_counts=True)).T
        locals()[name] = np.hstack(
            [locals()[name], locals()[name][:,1].reshape(-1,1)/np.sum(locals()[name][:,1])]
        )
        locals()[name] = pd.DataFrame(locals()[name], columns=['class', 'count', 'proportion'])
        locals()[name][['class', 'count']] = locals()[name][['class', 'count']].astype(int)
        locals()[name] = locals()[name].set_index('class')

    return locals()[frame_name_list[0]], locals()[frame_name_list[1]]


def make_conf_matrix(predictions, y_actual):
    """
    Generates a confusion matrix of actual classes versus predicted classes
    
    predictions: np.array, predicted class values generated by the model
    y_actual: np.array, actual class values
    
    returns: pd.DataFrame, the resulting confusion matrix with marginal totals included
    """
    class_labels = sorted(list(set(y_actual)))
    
    conf_matrix = pd.DataFrame(confusion_matrix(predictions, y_actual), columns=class_labels)
    conf_matrix.index.name = 'Predicted'
    conf_matrix.columns.name = 'Actual'
    conf_matrix.loc['Total'] = conf_matrix.sum(axis=0)
    conf_matrix['Total'] = conf_matrix.sum(axis=1)
    
    return conf_matrix


def generate_class_metrics(conf_matrix):
    """
    Generates a dataframe of the various by-class classification metrics that
    can be calculated directly from a confusion matrix, such as True Positive Rate,
    False Negative Rate, etc.
    
    conf_matrix: pd.DataFrame, generated with the make_conf_matrix function
    
    returns: pd.DataFrame, each row representing a different class, each
             column a different classification metric for each class
    """
    # rename index
    conf_matrix = conf_matrix.copy()
    conf_matrix.index.name = 'class'
    conf_matrix.columns.name = 'class'
    results = dict()
    
    # True Pos, False Pos, False Neg, True Neg counts
    results['TP'] = pd.Series(np.diag(conf_matrix.iloc[:-1,:-1]))
    results['TP'].index.name = 'class'
    results['FP'] = conf_matrix.iloc[:-1,:-1].sum(axis=1) - results['TP']  
    results['FN'] = conf_matrix.iloc[:-1,:-1].sum(axis=0) - results['TP']
    results['TN'] = conf_matrix.iloc[:-1,:-1].values.sum() -\
                    (results['FP'] + results['FN'] + results['TP'])

    # true positive rate
    results['TPR'] = results['TP']/(results['TP']+results['FN'])
    # false negative rate
    results['FNR'] = results['FN']/(results['TP']+results['FN'])
    # false positive rate
    results['FPR'] = results['FP']/(results['FP']+results['TN'])
    # true negative rate
    results['TNR'] = results['TN']/(results['TN']+results['FP']) 
    # positive predictive value
    results['PPV'] = results['TP']/(results['TP']+results['FP'])
    # negative predictive value
    results['NPV'] = results['TN']/(results['TN']+results['FN'])
    
    # convert results dictionary to a dataframe 
    results_df = pd.concat(
        [
            # first separate the integer values so they format correctly
            pd.DataFrame(
                list(results.values())[:4],
                index=list(results.keys())[:4]
            ).T,
            # followed by the float values
            pd.DataFrame(
                list(results.values())[4:],
                index=list(results.keys())[4:]
            ).T
        ], axis=1
    )
    
    return results_df


def generate_roc_auc(y_values_actual, predicted_probabilities, class_dict):
    """
    Creates a dictionary of ROC curve values generated using
    sklearn.metrics.roc_curve for every outcome class in a multi-class
    problem
    
    NOTE: multi-class AUC requires scikit-learn>=v0.22

    y_values_actual: np.array, the 1-dimensional array containing the 
                     multi-classs true y values against which you are evaluating
                     the predicted probabilities (i.e. y_test)
    predicted_probabilities: np.array, the 2-dimensional array generated
                             using sklearn's "model.predict_proba()" method
                             (i.e. test set predicted probabilities)
                     
    returns: tuple(float, float, dict), (1) a float representing the average macro AUC
             for all classes, (2) a float representing the average weighted AUC (weighted
             by the number of true samples for each class to account for class imbalance)
             and (3) a dictionary of dictionaries, where each top level key represents a
             different y class, and the value for each y class key is a dictionary
             containing the corresponding frp, tpr, threshold, and individual class AUC
             values for that particular y class outcome. Example output format shown below:
             
             (
                 auc_average,
                 auc_weighted_average,
                 output_dict = {
                    0: {
                        'frp': np.array of shape (n,)
                        'tpr': np.array of shape (n,)
                        'threshold': np.array of shape (n,)
                        'auc': float of micro auc for individual class
                        'name': str name of class
                    }
                    1: {
                        'frp': ...
                        ...
                    }
                    ...
                 }
            )
    """
    # create sorted list of all class labels
    class_labels = sorted(list(set(y_values_actual)))

    # convert y_values to binary indicators for each class and store as 2D
    # array of dimensions (n_classes, n_y_values), with each row containing one
    # set of class indicators
    y_class_array = np.vstack(
        [
            (y_values_actual==class_val).astype(int) for class_val in class_labels
        ]
    )

    # create roc curve dictionary
    roc_curve_dict = {
        crime_class: {
            key: value
            for key, value in zip(
                ['fpr', 'tpr', 'thresholds'],
                roc_curve(y_class, predicted_probs_class)
            )
        } for (crime_class, predicted_probs_class), y_class in zip(
            enumerate(predicted_probabilities.T),
            y_class_array
        )
    }
    
    # add individual class auc's and class names to dictionary
    for crime_class in class_labels:
        roc_curve_dict[crime_class]['auc'] = roc_auc_score(
            y_class_array[crime_class],
            predicted_probabilities[:,crime_class]
        )
        roc_curve_dict[crime_class]['name'] = class_dict[crime_class]
    
    # generate overall average auc's for all classes, weighted and unweighted
    auc_avg = roc_auc_score(
        y_values_actual, predicted_probabilities, multi_class='ovr', average='macro'
    )
    auc_weighted_avg = roc_auc_score(
        y_values_actual, predicted_probabilities, multi_class='ovr', average='weighted'
    )
    
    return auc_avg, auc_weighted_avg, roc_curve_dict


def generate_model_dict(classifier, X_train, X_test, y_train, y_test, class_dict,
                    verbose=False, roc_auc=True, **kwargs):
    """
    Fits the specified scikit-learn classifier type and generates a dictionary of results
    
    classifier: the uninitiated sklearn classification model object you wish
           to use (e.g. LogisticRegression, KNeighborsClassifier)
    X_train, X_test, y_train, y_test: the datasets on which to fit and
                                      evaluate the model
    class_dict: dict, key values must be the class number (i.e 0, 1, 2, ...) and
                the corresponding values must be the class name string (i.e. 'other',
                'burlary', ...) for each respective class number
    verbose: if True prints resulting fitted model object
    roc_auc: if True calculates and stores roc and auc dictionaries for both train and test
    **kwargs: are optional classifier-specific arguments that pass directly to the model
              while fitting

    return: returns a dictionary object containing the resulting fitted model object,
            resulting predictions, predicted probabilities, prediction count summary tables,
            confusion matrices, accuracy scores, and (if roc_auc=True) the AUC, weighted AUC,
            and ROC AUC dictionary for both the training and test sets
    """
    # Fit model with parameters specified by kwargs
    FitModel = classifier(**kwargs).fit(X_train, y_train)

    # generate and save predictions on both train and test data
    train_pred = FitModel.predict(X_train)
    test_pred = FitModel.predict(X_test)
    
    # generate and save prediction summary tables for both train and test predictions
    pred_counts_train, pred_counts_test = summarize_predictions(
        train_pred, test_pred
    )
    
    # generate confusion matrices
    conf_matrix_train = make_conf_matrix(train_pred, y_train)
    conf_matrix_test = make_conf_matrix(test_pred, y_test)
    
    # store fitted model, predictions and accuracy scores to dict 
    model_dict = {
        'model': FitModel,
        'predictions': {
            'train': train_pred,
            'test': test_pred,
        },
        'probabilities': {
            'train': FitModel.predict_proba(X_train),
            'test': FitModel.predict_proba(X_test),
        },
        'pred_counts': {
            'train': pred_counts_train,
            'test': pred_counts_test,
        },
        'conf_matrix': {
            'train': conf_matrix_train,
            'test': conf_matrix_test,
        },
        'class_metrics': {
            'train': generate_class_metrics(conf_matrix_train),
            'test': generate_class_metrics(conf_matrix_test),
        },
        'accuracy': {
            'train': accuracy_score(y_train, train_pred),
            'test': accuracy_score(y_test, test_pred),
        },
    }
    
    # generate roc and auc metrics for both train and test data if True
    if roc_auc:
        roc_auc_metrics = ['auc', 'auc_weighted', 'roc_auc_dict']
        roc_auc_train = generate_roc_auc(
                y_train, model_dict['probabilities']['train'],
                class_dict
        )
        roc_auc_test = generate_roc_auc(
                y_test, model_dict['probabilities']['test'],
                class_dict
        )
        # add roc and auc metrics to model_dict        
        for name, train_object, test_object in zip(
            roc_auc_metrics, roc_auc_train, roc_auc_test
        ):
            model_dict[name] = {
                'train': train_object,
                'test': test_object,
            }
    
    if verbose:
        print("\t{}".format(FitModel))

    return model_dict


def print_model_results(model_dict, accuracy='both', auc='both',
                        pred_counts='both', conf_matrix='both',
                        class_metrics='both'):
    """
    Prints a model results summary from the model dictionary generated
    using the generate_model_dict() function
    
    model_dict: dict, output dictionary from generate_model_dict() function
    accuracy: None, 'both', 'test', or 'train' parameters accepted,
              identifies which results to print for this particular metric
    auc: same parameters accepted
    pred_counts: same parameters accepted
    conf_matrix: same parameters accepted
    class_metrics: same parameters accepted
    
    returns: nothing is returned, this function just prints summary output
    """
    train_opt = ['train', 'both']
    test_opt = ['test', 'both']
    
    print('\nThe fitted model:\n\n{}\n\n'.format(model_dict['model']))
    
    if accuracy:
        print('This model resulted in the following accuracy:\n')
        if accuracy in train_opt:
            print('Training\t{:.4f}'.format(model_dict['accuracy']['train']))
        if accuracy in test_opt:
            print('Test\t\t{:.4f}'.format(model_dict['accuracy']['test']))
        print('\n')
            
    if auc:
        print('The model AUC is:\n\n\t\tweighted\tunweighted')
        if auc in train_opt:
            print(
                'Training\t{:.4f}\t\t{:.4f}'.format(
                    model_dict['auc_weighted']['train'],
                    model_dict['auc']['train']
                )
            )
        if auc in test_opt:
            print(
                'Test\t\t{:.4f}\t\t{:.4f}'.format(
                    model_dict['auc_weighted']['test'],
                    model_dict['auc']['test']
                )
            )
        print('\n')
        
    if pred_counts:
        print('The number of classes predicted by class are:\n')
        if pred_counts in train_opt:
            print('TRAINING\n{}\n'.format(model_dict['pred_counts']['train']))
        if pred_counts in test_opt:
            print('TEST\n{}\n'.format(model_dict['pred_counts']['test']))
        print()
    
    if conf_matrix:
        print('The resulting confusion matrix:\n')
        if conf_matrix in train_opt:
            print('TRAINING\n{}\n'.format(model_dict['conf_matrix']['train']))
        if conf_matrix in test_opt:
            print('TEST\n{}\n'.format(model_dict['conf_matrix']['test']))
        print()
        
    if class_metrics:
        print('The classification metrics derived from the confusion matrix are:\n')
        if class_metrics in train_opt:
            print('TRAINING\n{}\n'.format(model_dict['class_metrics']['train'].iloc[:,:8]))
        if class_metrics in test_opt:
            print('TEST\n{}\n'.format(model_dict['class_metrics']['test'].iloc[:,:8]))
        print('\n')


def generate_save_load_model_dict(model_dict_name, target_directory,
                                  classifier, X_train, X_test, y_train, y_test,
                                  class_dict, verbose=False, roc_auc=True, overwrite=False,
                                  **kwargs):
    """
    Prior to running the generate_model_dict() function, this first
    checks to determine whether a saved copy of the specified model has
    already been saved to the specified path. 
    
    If no saved dict file is found the function calls generate_model_dict()
    and saves the resulting dictionary to the specified filepath.
    
    If a saved dict file does exist, the function simply loads that file
    instead of running the model again.
    
    NOTE: the resulting target filepath used for saving/checking for the resulting
          model_dict is generated as:
            
            os.path.join(target_directory, ''.join([model_dict_name, '.joblib']))
    
    model_dict_name: str, specifies the desired name of the model dict object you wish
                to generate
    target_directory: str, the directory path in which you wish to save or check for
                      the resulting model
    classifier: the uninitiated sklearn classification model object you wish
           to use (e.g. LogisticRegression, KNeighborsClassifier)
    X_train, X_test, y_train, y_test: the datasets on which to fit and
                                      evaluate the model
    class_dict: dict, key values must be the class number (i.e 0, 1, 2, ...) and
                the corresponding values must be the class name string (i.e. 'other',
                'burlary', ...) for each respective class number
    verbose: if True prints resulting fitted model object
    roc_auc: if True calculates and stores roc and auc dictionaries for both train
             and test
    overwrite: bool, default=False, if True, generate_model_dict() will generate a new
               dictionary regardless of whether or not the target filepath already
               exists and overwrite any file that already exists at the specified
               filepath
    **kwargs: are optional classifier-specific arguments that pass directly to the model
              while fitting

    returns: dict, dictionary object containing the resulting fitted model object,
             resulting predictions, predicted probabilities, prediction count summary tables,
             confusion matrices, accuracy scores, and (if roc_auc=True) the AUC, weighted AUC,
             and ROC AUC dictionary for both the training and test sets
    """
    filepath = os.path.join(target_directory, ''.join([model_dict_name, '.joblib']))
    
    if os.path.exists(filepath) and not overwrite:
        locals()[model_dict_name] = joblib.load(filepath)
        print(
            '\nThe model dictionary already exists and has been LOADED from:'\
            '\n\n\t{}\n'.format(filepath)
        )
    else:
        locals()[model_dict_name] = generate_model_dict(
            classifier,
            X_train,
            X_test,
            y_train,
            y_test,
            class_dict,
            verbose,
            roc_auc,
            **kwargs
        )
        dump_loc = joblib.dump(locals()[model_dict_name], filepath)
        print(
            '\nThe new model dictionary has been generated and SAVED to disk at:'\
            '\n\n\t{}\n'.format(dump_loc[0])
        )
    
    return locals()[model_dict_name]

