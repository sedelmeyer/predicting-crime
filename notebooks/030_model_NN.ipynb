{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Notebook 030: Neural Networks\n",
    "\n",
    "This notebook contains a baseline Neural Network for the task of predicting crime types. It takes a while to run so we recommend to just look at the output\n",
    "\n",
    "## Skip to section 2 for the actual work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:45.071607Z",
     "start_time": "2019-12-11T17:41:44.961665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import operator\n",
    "import pathlib\n",
    "import requests\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:45.215478Z",
     "start_time": "2019-12-11T17:41:45.190878Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readfile paths for datasets used in this notebook are:\n",
      "\t../data/processed/X_train.csv\n",
      "\t../data/raw/shapefile/census-tracts/Census_2010_Tracts.shp\n",
      "\t../figures/model-logistic\n",
      "\t../models/logistic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '../data/'\n",
    "FIGURES_ROOT = '../figures/model-logistic'\n",
    "WRITEDIR_ROOT = '../models/logistic'\n",
    "\n",
    "READDIR_ROOT = os.path.join(DATA_ROOT, 'processed')\n",
    "SHAPEDIR_ROOT = os.path.join(DATA_ROOT, 'raw')\n",
    "\n",
    "readfile_model_X = os.path.join(READDIR_ROOT, 'X_train.csv')\n",
    "readfile_model_X_scaled = os.path.join(READDIR_ROOT, 'X_train_scaled.csv')\n",
    "readfile_model_y = os.path.join(READDIR_ROOT, 'y_train.csv')\n",
    "readfile_model_X_test = os.path.join(READDIR_ROOT, 'X_test.csv')\n",
    "readfile_model_X_test_scaled = os.path.join(READDIR_ROOT, 'X_test_scaled.csv')\n",
    "readfile_model_y_test = os.path.join(READDIR_ROOT, 'y_test.csv')\n",
    "readfile_model_X_scaler = os.path.join(READDIR_ROOT, 'X_scaler.csv')\n",
    "\n",
    "readfile_zipshapes = os.path.join(SHAPEDIR_ROOT, 'shapefile/zipcodes/ZIP_Codes.shp')\n",
    "readfile_cityshape = os.path.join(SHAPEDIR_ROOT, 'shapefile/city-boundary/City_of_Boston_Boundary.shp')\n",
    "readfile_streetshapes = os.path.join(SHAPEDIR_ROOT, 'shapefile/street-segments/Boston_Street_Segments.shp')\n",
    "readfile_tractshapes = os.path.join(SHAPEDIR_ROOT, 'shapefile/census-tracts/Census_2010_Tracts.shp')\n",
    "readfile_hoodshapes = os.path.join(SHAPEDIR_ROOT, 'shapefile/boston-neighborhoods/Boston_Neighborhoods.shp')\n",
    "readfile_zonesubshapes = os.path.join(SHAPEDIR_ROOT, 'shapefile/zoning-subdistricts/Zoning_Subdistricts.shp')\n",
    "readfile_openshapes = os.path.join(SHAPEDIR_ROOT, 'shapefile/open-spaces/Open_Space.shp')\n",
    "\n",
    "print(\n",
    "    'readfile paths for datasets used in this notebook are:\\n\\t{}\\n\\t{}\\n\\t{}\\n\\t{}\\n'.format(\n",
    "        readfile_model_X, readfile_tractshapes, FIGURES_ROOT, WRITEDIR_ROOT\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:45.491063Z",
     "start_time": "2019-12-11T17:41:45.482102Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mkdir for saving figures if it doesn't already exist\n",
    "if not os.path.exists(FIGURES_ROOT):\n",
    "    os.mkdir(FIGURES_ROOT)\n",
    "    \n",
    "# mkdir for saving output data if it doesn't already exist\n",
    "if not os.path.exists(WRITEDIR_ROOT):\n",
    "    os.mkdir(WRITEDIR_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Read labeled training and TEST data and subset predictors and response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:51.655888Z",
     "start_time": "2019-12-11T17:41:46.194819Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read in labeled model data\n",
    "X_train_labels = pd.read_csv(readfile_model_X)\n",
    "X_train_scaled_labels = pd.read_csv(readfile_model_X_scaled)\n",
    "y_train_labels = pd.read_csv(readfile_model_y)\n",
    "\n",
    "X_test_labels = pd.read_csv(readfile_model_X_test)\n",
    "X_test_scaled_labels = pd.read_csv(readfile_model_X_test_scaled)\n",
    "y_test_labels = pd.read_csv(readfile_model_y_test)\n",
    "\n",
    "# read in scaled data scaling parameters to restore scaled values if needed\n",
    "X_scaler = pd.read_csv(readfile_model_X_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:51.907174Z",
     "start_time": "2019-12-11T17:41:51.659926Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# subset X and Y data to include only the columns used for model training\n",
    "predictor_columns = list(X_train_labels.iloc[:,17:].columns)\n",
    "response_column = ['crime-type']\n",
    "\n",
    "X_train = X_train_labels[predictor_columns].copy()\n",
    "X_train_scaled = X_train_scaled_labels[predictor_columns].copy()\n",
    "y_train = y_train_labels[response_column].values.ravel()\n",
    "\n",
    "X_test = X_test_labels[predictor_columns].copy()\n",
    "X_test_scaled = X_test_scaled_labels[predictor_columns].copy()\n",
    "y_test = y_test_labels[response_column].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:52.054721Z",
     "start_time": "2019-12-11T17:41:51.914807Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y TRAIN DATA WITH ADDITIONAL LABELS\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128352 entries, 0 to 128351\n",
      "Data columns (total 4 columns):\n",
      "crime-type             128352 non-null int64\n",
      "crime-type-cat         128352 non-null object\n",
      "OFFENSE_DESCRIPTION    128352 non-null object\n",
      "INCIDENT_NUMBER        128352 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.9+ MB\n",
      "\n",
      "\n",
      "X TRAIN DATA WITH ADDITIONAL LABELS\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128352 entries, 0 to 128351\n",
      "Data columns (total 60 columns):\n",
      "date                                 128352 non-null object\n",
      "year                                 128352 non-null int64\n",
      "month                                128352 non-null int64\n",
      "hour                                 128352 non-null int64\n",
      "time                                 128352 non-null int64\n",
      "ZIP5                                 128352 non-null int64\n",
      "ZIP5_area                            128352 non-null float64\n",
      "Name                                 128352 non-null object\n",
      "Neighborhood_area                    128352 non-null float64\n",
      "Neighborhood_area_2                  128352 non-null float64\n",
      "TRACTCE10                            128352 non-null int64\n",
      "TRACTCE10_area                       128352 non-null float64\n",
      "TRACTCE10_area_2                     128352 non-null float64\n",
      "weathercodes                         62450 non-null object\n",
      "college-distance                     128352 non-null float64\n",
      "highschool-distance                  128352 non-null float64\n",
      "streetlights                         128352 non-null int64\n",
      "streetlights-night                   128352 non-null int64\n",
      "lat                                  128352 non-null float64\n",
      "lon                                  128352 non-null float64\n",
      "night                                128352 non-null int64\n",
      "tempavg                              128352 non-null float64\n",
      "windavg                              128352 non-null float64\n",
      "precip                               128352 non-null float64\n",
      "snowfall                             128352 non-null float64\n",
      "Tuesday                              128352 non-null int64\n",
      "Wednesday                            128352 non-null int64\n",
      "Thursday                             128352 non-null int64\n",
      "Friday                               128352 non-null int64\n",
      "Saturday                             128352 non-null int64\n",
      "Sunday                               128352 non-null int64\n",
      "Feb                                  128352 non-null int64\n",
      "Mar                                  128352 non-null int64\n",
      "Apr                                  128352 non-null int64\n",
      "May                                  128352 non-null int64\n",
      "Jun                                  128352 non-null int64\n",
      "Jul                                  128352 non-null int64\n",
      "Aug                                  128352 non-null int64\n",
      "Sep                                  128352 non-null int64\n",
      "Oct                                  128352 non-null int64\n",
      "Nov                                  128352 non-null int64\n",
      "Dec                                  128352 non-null int64\n",
      "college-near                         128352 non-null int64\n",
      "highschool-near                      128352 non-null int64\n",
      "median-age                           128352 non-null float64\n",
      "median-income                        128352 non-null float64\n",
      "poverty-rate                         128352 non-null float64\n",
      "less-than-high-school-perc           128352 non-null float64\n",
      "bachelor-degree-or-more-perc         128352 non-null float64\n",
      "enrolled-college-perc                128352 non-null float64\n",
      "commercial-mix-ratio                 128352 non-null float64\n",
      "industrial-mix-ratio                 128352 non-null float64\n",
      "owner-occupied-ratio                 128352 non-null float64\n",
      "residential-median-value             128352 non-null float64\n",
      "residential-gini-coef                128352 non-null float64\n",
      "commercial-mix-ratio-3yr-cagr        128352 non-null float64\n",
      "industrial-mix-ratio-3yr-cagr        128352 non-null float64\n",
      "owner-occupied-ratio-3yr-cagr        128352 non-null float64\n",
      "residential-gini-coef-3yr-cagr       128352 non-null float64\n",
      "residential-median-value-3yr-cagr    128352 non-null float64\n",
      "dtypes: float64(29), int64(28), object(3)\n",
      "memory usage: 58.8+ MB\n",
      "\n",
      "\n",
      "X TRAIN PREDICTORS (WITH LABEL COLUMNS EXCLUDED)\n",
      "\n",
      "['streetlights-night', 'lat', 'lon', 'night', 'tempavg', 'windavg', 'precip', 'snowfall', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'college-near', 'highschool-near', 'median-age', 'median-income', 'poverty-rate', 'less-than-high-school-perc', 'bachelor-degree-or-more-perc', 'enrolled-college-perc', 'commercial-mix-ratio', 'industrial-mix-ratio', 'owner-occupied-ratio', 'residential-median-value', 'residential-gini-coef', 'commercial-mix-ratio-3yr-cagr', 'industrial-mix-ratio-3yr-cagr', 'owner-occupied-ratio-3yr-cagr', 'residential-gini-coef-3yr-cagr', 'residential-median-value-3yr-cagr']\n",
      "\n",
      "\n",
      "THE SHAPE OF THE X AND Y TRAIN FRAMES WITH PREDICTORS ONLY\n",
      "\n",
      "(128352, 43)\n",
      "(128352, 43)\n",
      "(128352,)\n",
      "\n",
      "\n",
      "X SCALER DATA INFO\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26 entries, 0 to 25\n",
      "Data columns (total 3 columns):\n",
      "scaled-feature    26 non-null object\n",
      "std               26 non-null float64\n",
      "mean              26 non-null float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 752.0+ bytes\n",
      "\n",
      "\n",
      "THE X SCALER DATA CONTAINS MEANS AND STDS FOR THE FOLLOWING STANDARDIZED (i.e. SCALED) PREDICTORS\n",
      "\n",
      "['bachelor-degree-or-more-perc' 'college-distance' 'commercial-mix-ratio'\n",
      " 'commercial-mix-ratio-3yr-cagr' 'enrolled-college-perc'\n",
      " 'highschool-distance' 'industrial-mix-ratio'\n",
      " 'industrial-mix-ratio-3yr-cagr' 'lat' 'less-than-high-school-perc' 'lon'\n",
      " 'median-age' 'median-income' 'owner-occupied-ratio'\n",
      " 'owner-occupied-ratio-3yr-cagr' 'poverty-rate' 'precip'\n",
      " 'residential-gini-coef' 'residential-gini-coef-3yr-cagr'\n",
      " 'residential-median-value' 'residential-median-value-3yr-cagr' 'snowfall'\n",
      " 'streetlights' 'tempavg' 'windavg' 'streetlights-night']\n"
     ]
    }
   ],
   "source": [
    "# summarize dataframe info\n",
    "print('Y TRAIN DATA WITH ADDITIONAL LABELS\\n')\n",
    "y_train_labels.info()\n",
    "print('\\n\\nX TRAIN DATA WITH ADDITIONAL LABELS\\n')\n",
    "X_train_labels.info()\n",
    "print('\\n\\nX TRAIN PREDICTORS (WITH LABEL COLUMNS EXCLUDED)\\n')\n",
    "print(predictor_columns)\n",
    "print('\\n\\nTHE SHAPE OF THE X AND Y TRAIN FRAMES WITH PREDICTORS ONLY\\n')\n",
    "print(X_train.shape)\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print('\\n\\nX SCALER DATA INFO\\n')\n",
    "X_scaler.info()\n",
    "print(\n",
    "    '\\n\\nTHE X SCALER DATA CONTAINS MEANS AND STDS FOR THE FOLLOWING STANDARDIZED '\\\n",
    "    '(i.e. SCALED) PREDICTORS\\n\\n{}'.format(X_scaler['scaled-feature'].values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:52.073528Z",
     "start_time": "2019-12-11T17:41:52.061539Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary predictors in the X training data are:\n",
      "\n",
      "['night', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'college-near', 'highschool-near']\n",
      "\n",
      "\n",
      "Non-binary predictors are:\n",
      "\n",
      "['streetlights-night', 'lat', 'lon', 'tempavg', 'windavg', 'precip', 'snowfall', 'median-age', 'median-income', 'poverty-rate', 'less-than-high-school-perc', 'bachelor-degree-or-more-perc', 'enrolled-college-perc', 'commercial-mix-ratio', 'industrial-mix-ratio', 'owner-occupied-ratio', 'residential-median-value', 'residential-gini-coef', 'commercial-mix-ratio-3yr-cagr', 'industrial-mix-ratio-3yr-cagr', 'owner-occupied-ratio-3yr-cagr', 'residential-gini-coef-3yr-cagr', 'residential-median-value-3yr-cagr']\n",
      "\n",
      "\n",
      "Of the non-binary, the following are discrete:\n",
      "\n",
      "['streetlights-night', 'median-age']\n",
      "\n",
      "\n",
      "And these remaining are continuous:\n",
      "\n",
      "['lat', 'lon', 'tempavg', 'windavg', 'precip', 'snowfall', 'median-income', 'poverty-rate', 'less-than-high-school-perc', 'bachelor-degree-or-more-perc', 'enrolled-college-perc', 'commercial-mix-ratio', 'industrial-mix-ratio', 'owner-occupied-ratio', 'residential-median-value', 'residential-gini-coef', 'commercial-mix-ratio-3yr-cagr', 'industrial-mix-ratio-3yr-cagr', 'owner-occupied-ratio-3yr-cagr', 'residential-gini-coef-3yr-cagr', 'residential-median-value-3yr-cagr']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create lists of predictors based on numeric type for easier treatment of specific types\n",
    "predictors_binary = [\n",
    "    'night', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
    "    'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', \n",
    "    'college-near', 'highschool-near'\n",
    "]\n",
    "predictors_nonbinary = [col for col in X_train.columns if col not in predictors_binary]\n",
    "predictors_discrete = [\n",
    "    'streetlights-night',\n",
    "    'median-age',\n",
    "]\n",
    "predictors_continuous = [col for col in predictors_nonbinary if col not in predictors_discrete]\n",
    "\n",
    "print(\n",
    "    'Binary predictors in the X training data are:\\n\\n{}\\n\\n\\n'\\\n",
    "    'Non-binary predictors are:\\n\\n{}\\n\\n\\n'\\\n",
    "    'Of the non-binary, the following are discrete:\\n\\n{}\\n\\n\\n'\\\n",
    "    'And these remaining are continuous:\\n\\n{}\\n'.format(\n",
    "        predictors_binary,\n",
    "        predictors_nonbinary,\n",
    "        predictors_discrete,\n",
    "        predictors_continuous\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Inspect multi-collinearity of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:52.088207Z",
     "start_time": "2019-12-11T17:41:52.078233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sort_pairwise_correlation(X_train):\n",
    "    \"\"\"\n",
    "    Calculates a correlation matrix of all input predictors and observations,\n",
    "    via the X_train dataframe, then returns a sorted dataframe of all\n",
    "    pairwise correlation values (duplicates and same-value pairings removed).\n",
    "    \n",
    "    The returned sorted correlation summary dataframe is sorted in descending\n",
    "    order, from highest to lowest correlation pairings.\n",
    "    \"\"\"\n",
    "    # calculate correlation among all predictors\n",
    "    correlation_matrix_df = X_train.corr()\n",
    "    correlation_abs_matrix_df = X_train.corr().abs()\n",
    "\n",
    "    # identify variables most heavily correlated in descending order\n",
    "    # and remove pairwise groupings of the same predictors\n",
    "    corr_unstack = correlation_abs_matrix_df.unstack()\n",
    "    corr_sorted = corr_unstack.sort_values(ascending=False)\n",
    "    corr_sorted = corr_sorted[corr_sorted < 1][::2]\n",
    "    \n",
    "    return corr_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.164184Z",
     "start_time": "2019-12-11T17:41:52.090796Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The most strongly correlated predictors (corr > 0.50) in our baseline model predictor set and their corresponding correlation values are:\n",
      "\n",
      "poverty-rate                  median-income                   0.832140\n",
      "bachelor-degree-or-more-perc  less-than-high-school-perc      0.829621\n",
      "night                         streetlights-night              0.814221\n",
      "enrolled-college-perc         bachelor-degree-or-more-perc    0.775921\n",
      "owner-occupied-ratio          enrolled-college-perc           0.735861\n",
      "median-age                    owner-occupied-ratio            0.726905\n",
      "residential-gini-coef         residential-median-value        0.719481\n",
      "median-income                 less-than-high-school-perc      0.718383\n",
      "bachelor-degree-or-more-perc  median-income                   0.714370\n",
      "                              residential-gini-coef           0.672335\n",
      "college-near                  enrolled-college-perc           0.670402\n",
      "poverty-rate                  median-age                      0.668691\n",
      "enrolled-college-perc         residential-gini-coef           0.663297\n",
      "bachelor-degree-or-more-perc  residential-median-value        0.628928\n",
      "median-age                    enrolled-college-perc           0.623947\n",
      "lat                           enrolled-college-perc           0.620707\n",
      "college-near                  residential-gini-coef           0.611998\n",
      "owner-occupied-ratio          lat                             0.610437\n",
      "residential-median-value      commercial-mix-ratio            0.610041\n",
      "residential-gini-coef         commercial-mix-ratio            0.604891\n",
      "college-near                  bachelor-degree-or-more-perc    0.583419\n",
      "lat                           bachelor-degree-or-more-perc    0.562123\n",
      "residential-median-value      college-near                    0.556280\n",
      "lat                           residential-gini-coef           0.555159\n",
      "residential-median-value      enrolled-college-perc           0.554235\n",
      "enrolled-college-perc         commercial-mix-ratio            0.549602\n",
      "median-income                 residential-median-value        0.523196\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "## Calculate and report on pairwise correlations\n",
    "#############################################\n",
    "\n",
    "# generate sorted correlation dataframe\n",
    "corr_df = sort_pairwise_correlation(X_train)\n",
    "\n",
    "# set corr threshold for reporting\n",
    "corr_thresh = 0.50\n",
    "\n",
    "print(\n",
    "    '\\nThe most strongly correlated predictors (corr > {0:.2f}) in our baseline model '\\\n",
    "    'predictor set and their corresponding correlation values are:\\n\\n{1}\\n'.format(\n",
    "        corr_thresh,\n",
    "        corr_df[corr_df > corr_thresh],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create value count summary tables by crime-type for later reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.343681Z",
     "start_time": "2019-12-11T17:41:54.184727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agg_dict = {'INCIDENT_NUMBER': 'count'}\n",
    "\n",
    "crime_types_train = y_train_labels.groupby(\n",
    "    ['crime-type', 'crime-type-cat']\n",
    ").agg(agg_dict).rename(columns=agg_dict)\n",
    "\n",
    "crime_types_train['proportion'] = crime_types_train / crime_types_train.sum()\n",
    "\n",
    "crime_types_test = y_test_labels.groupby(\n",
    "    ['crime-type', 'crime-type-cat']\n",
    ").agg(agg_dict).rename(columns=agg_dict)\n",
    "\n",
    "crime_types_test['proportion'] = crime_types_test / crime_types_test.sum()\n",
    "\n",
    "# create dictionary of crime types for later use\n",
    "class_dict = dict(\n",
    "    zip(crime_types_test.reset_index()['crime-type'], crime_types_test.reset_index()['crime-type-cat'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.355859Z",
     "start_time": "2019-12-11T17:41:54.345963Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'other',\n",
       " 1: 'burglary',\n",
       " 2: 'drugs-substances',\n",
       " 3: 'fraud',\n",
       " 4: 'harassment-disturbance',\n",
       " 5: 'robbery',\n",
       " 6: 'theft',\n",
       " 7: 'vandalism-property',\n",
       " 8: 'violence-aggression'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.369970Z",
     "start_time": "2019-12-11T17:41:54.358921Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observed crime type classes in both the training and test set are:\n",
      "\n",
      "TRAINING\n",
      "                                   count  proportion\n",
      "crime-type crime-type-cat                           \n",
      "0          other                    6321    0.049247\n",
      "1          burglary                 5664    0.044129\n",
      "2          drugs-substances        13082    0.101923\n",
      "3          fraud                    9587    0.074693\n",
      "4          harassment-disturbance  20767    0.161797\n",
      "5          robbery                  3423    0.026669\n",
      "6          theft                   34555    0.269221\n",
      "7          vandalism-property      13710    0.106816\n",
      "8          violence-aggression     21243    0.165506\n",
      "\n",
      "\n",
      "TEST\n",
      "                                   count  proportion\n",
      "crime-type crime-type-cat                           \n",
      "0          other                    1580    0.049240\n",
      "1          burglary                 1416    0.044129\n",
      "2          drugs-substances         3271    0.101938\n",
      "3          fraud                    2397    0.074701\n",
      "4          harassment-disturbance   5192    0.161805\n",
      "5          robbery                   856    0.026677\n",
      "6          theft                    8639    0.269228\n",
      "7          vandalism-property       3427    0.106800\n",
      "8          violence-aggression      5310    0.165482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'The number of observed crime type classes in both the training and test set are:\\n\\n'\\\n",
    "    'TRAINING\\n{}\\n\\n\\nTEST\\n{}\\n'.format(\n",
    "        crime_types_train,\n",
    "        crime_types_test\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Define functions used to generate and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.439547Z",
     "start_time": "2019-12-11T17:41:54.373760Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# DEFINE FUNCTIONS THAT CONTRIBUTE TO GENERATING A FITTED MODEL DICT \n",
    "####################################################################\n",
    "\n",
    "def summarize_predictions(predictions_train, predictions_test):\n",
    "    \"\"\"\n",
    "    Generates a summary table of the training and test predictions, showing count\n",
    "    and proportion by class\n",
    "    \n",
    "    predictions_train: np.array, the predicted classes generated on the training data\n",
    "    predictions_test:np.array, the predicted classes generated on the test data\n",
    "    \n",
    "    returns: tuple of 2 pd.DataFrame objects, one summarizing the training predictions\n",
    "             the other summarizing the test predictions\n",
    "    \"\"\"\n",
    "    frame_name_list = ['pred_counts_train', 'pred_counts_test']\n",
    "    \n",
    "    for predictions, name in zip(\n",
    "        [predictions_train, predictions_test],\n",
    "        frame_name_list\n",
    "    ):\n",
    "        locals()[name] = np.array(np.unique(predictions, return_counts=True)).T\n",
    "        locals()[name] = np.hstack(\n",
    "            [locals()[name], locals()[name][:,1].reshape(-1,1)/np.sum(locals()[name][:,1])]\n",
    "        )\n",
    "        locals()[name] = pd.DataFrame(locals()[name], columns=['class', 'count', 'proportion'])\n",
    "        locals()[name][['class', 'count']] = locals()[name][['class', 'count']].astype(int)\n",
    "        locals()[name] = locals()[name].set_index('class')\n",
    "\n",
    "    return locals()[frame_name_list[0]], locals()[frame_name_list[1]]\n",
    "\n",
    "\n",
    "def make_conf_matrix(predictions, y_actual):\n",
    "    \"\"\"\n",
    "    Generates a confusion matrix of actual classes versus predicted classes\n",
    "    \n",
    "    predictions: np.array, predicted class values generated by the model\n",
    "    y_actual: np.array, actual class values\n",
    "    \n",
    "    returns: pd.DataFrame, the resulting confusion matrix with marginal totals included\n",
    "    \"\"\"\n",
    "    class_labels = sorted(list(set(y_actual)))\n",
    "    \n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(predictions, y_actual), columns=class_labels)\n",
    "    conf_matrix.index.name = 'Predicted'\n",
    "    conf_matrix.columns.name = 'Actual'\n",
    "    conf_matrix.loc['Total'] = conf_matrix.sum(axis=0)\n",
    "    conf_matrix['Total'] = conf_matrix.sum(axis=1)\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "\n",
    "def generate_class_metrics(conf_matrix):\n",
    "    \"\"\"\n",
    "    Generates a dataframe of the various by-class classification metrics that\n",
    "    can be calculated directly from a confusion matrix, such as True Positive Rate,\n",
    "    False Negative Rate, etc.\n",
    "    \n",
    "    conf_matrix: pd.DataFrame, generated with the make_conf_matrix function\n",
    "    \n",
    "    returns: pd.DataFrame, each row representing a different class, each\n",
    "             column a different classification metric for each class\n",
    "    \"\"\"\n",
    "    # rename index\n",
    "    conf_matrix = conf_matrix.copy()\n",
    "    conf_matrix.index.name = 'class'\n",
    "    conf_matrix.columns.name = 'class'\n",
    "    results = dict()\n",
    "    \n",
    "    # True Pos, False Pos, False Neg, True Neg counts\n",
    "    results['TP'] = pd.Series(np.diag(conf_matrix.iloc[:-1,:-1]))\n",
    "    results['TP'].index.name = 'class'\n",
    "    results['FP'] = conf_matrix.iloc[:-1,:-1].sum(axis=1) - results['TP']  \n",
    "    results['FN'] = conf_matrix.iloc[:-1,:-1].sum(axis=0) - results['TP']\n",
    "    results['TN'] = conf_matrix.iloc[:-1,:-1].values.sum() -\\\n",
    "                    (results['FP'] + results['FN'] + results['TP'])\n",
    "\n",
    "    # true positive rate\n",
    "    results['TPR'] = results['TP']/(results['TP']+results['FN'])\n",
    "    # false negative rate\n",
    "    results['FNR'] = results['FN']/(results['TP']+results['FN'])\n",
    "    # false positive rate\n",
    "    results['FPR'] = results['FP']/(results['FP']+results['TN'])\n",
    "    # true negative rate\n",
    "    results['TNR'] = results['TN']/(results['TN']+results['FP']) \n",
    "    # positive predictive value\n",
    "    results['PPV'] = results['TP']/(results['TP']+results['FP'])\n",
    "    # negative predictive value\n",
    "    results['NPV'] = results['TN']/(results['TN']+results['FN'])\n",
    "    \n",
    "    # convert results dictionary to a dataframe \n",
    "    results_df = pd.concat(\n",
    "        [\n",
    "            # first separate the integer values so they format correctly\n",
    "            pd.DataFrame(\n",
    "                list(results.values())[:4],\n",
    "                index=list(results.keys())[:4]\n",
    "            ).T,\n",
    "            # followed by the float values\n",
    "            pd.DataFrame(\n",
    "                list(results.values())[4:],\n",
    "                index=list(results.keys())[4:]\n",
    "            ).T\n",
    "        ], axis=1\n",
    "    )\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def generate_roc_auc(y_values_actual, predicted_probabilities, class_dict):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of ROC curve values generated using\n",
    "    sklearn.metrics.roc_curve for every outcome class in a multi-class\n",
    "    problem\n",
    "    \n",
    "    NOTE: multi-class AUC requires scikit-learn>=v0.22\n",
    "\n",
    "    y_values_actual: np.array, the 1-dimensional array containing the \n",
    "                     multi-classs true y values against which you are evaluating\n",
    "                     the predicted probabilities (i.e. y_test)\n",
    "    predicted_probabilities: np.array, the 2-dimensional array generated\n",
    "                             using sklearn's \"model.predict_proba()\" method\n",
    "                             (i.e. test set predicted probabilities)\n",
    "                     \n",
    "    returns: tuple(float, float, dict), (1) a float representing the average macro AUC\n",
    "             for all classes, (2) a float representing the average weighted AUC (weighted\n",
    "             by the number of true samples for each class to account for class imbalance)\n",
    "             and (3) a dictionary of dictionaries, where each top level key represents a\n",
    "             different y class, and the value for each y class key is a dictionary\n",
    "             containing the corresponding frp, tpr, threshold, and individual class AUC\n",
    "             values for that particular y class outcome. Example output format shown below:\n",
    "             \n",
    "             (\n",
    "                 auc_average,\n",
    "                 auc_weighted_average,\n",
    "                 output_dict = {\n",
    "                    0: {\n",
    "                        'frp': np.array of shape (n,)\n",
    "                        'tpr': np.array of shape (n,)\n",
    "                        'threshold': np.array of shape (n,)\n",
    "                        'auc': float of micro auc for individual class\n",
    "                        'name': str name of class\n",
    "                    }\n",
    "                    1: {\n",
    "                        'frp': ...\n",
    "                        ...\n",
    "                    }\n",
    "                    ...\n",
    "                 }\n",
    "            )\n",
    "    \"\"\"\n",
    "    # create sorted list of all class labels\n",
    "    class_labels = sorted(list(set(y_values_actual)))\n",
    "\n",
    "    # convert y_values to binary indicators for each class and store as 2D\n",
    "    # array of dimensions (n_classes, n_y_values), with each row containing one\n",
    "    # set of class indicators\n",
    "    y_class_array = np.vstack(\n",
    "        [\n",
    "            (y_values_actual==class_val).astype(int) for class_val in class_labels\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # create roc curve dictionary\n",
    "    roc_curve_dict = {\n",
    "        crime_class: {\n",
    "            key: value\n",
    "            for key, value in zip(\n",
    "                ['fpr', 'tpr', 'thresholds'],\n",
    "                roc_curve(y_class, predicted_probs_class)\n",
    "            )\n",
    "        } for (crime_class, predicted_probs_class), y_class in zip(\n",
    "            enumerate(predicted_probabilities.T),\n",
    "            y_class_array\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # add individual class auc's and class names to dictionary\n",
    "    for crime_class in class_labels:\n",
    "        roc_curve_dict[crime_class]['auc'] = roc_auc_score(\n",
    "            y_class_array[crime_class],\n",
    "            predicted_probabilities[:,crime_class]\n",
    "        )\n",
    "        roc_curve_dict[crime_class]['name'] = class_dict[crime_class]\n",
    "    \n",
    "    # generate overall average auc's for all classes, weighted and unweighted\n",
    "    auc_avg = roc_auc_score(\n",
    "        y_values_actual, predicted_probabilities, multi_class='ovr', average='macro'\n",
    "    )\n",
    "    auc_weighted_avg = roc_auc_score(\n",
    "        y_values_actual, predicted_probabilities, multi_class='ovr', average='weighted'\n",
    "    )\n",
    "    \n",
    "    return auc_avg, auc_weighted_avg, roc_curve_dict\n",
    "\n",
    "\n",
    "def generate_model_dict(classifier, X_train, X_test, y_train, y_test, class_dict,\n",
    "                    verbose=False, roc_auc=True, fitm=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Fits the specified scikit-learn classifier type and generates a dictionary of results\n",
    "    \n",
    "    classifier: the uninitiated sklearn classification model object you wish\n",
    "           to use (e.g. LogisticRegression, KNeighborsClassifier)\n",
    "    X_train, X_test, y_train, y_test: the datasets on which to fit and\n",
    "                                      evaluate the model\n",
    "    class_dict: dict, key values must be the class number (i.e 0, 1, 2, ...) and\n",
    "                the corresponding values must be the class name string (i.e. 'other',\n",
    "                'burlary', ...) for each respective class number\n",
    "    verbose: if True prints resulting fitted model object\n",
    "    roc_auc: if True calculates and stores roc and auc dictionaries for both train and test\n",
    "    **kwargs: are optional classifier-specific arguments that pass directly to the model\n",
    "              while fitting\n",
    "\n",
    "    return: returns a dictionary object containing the resulting fitted model object,\n",
    "            resulting predictions, predicted probabilities, prediction count summary tables,\n",
    "            confusion matrices, accuracy scores, and (if roc_auc=True) the AUC, weighted AUC,\n",
    "            and ROC AUC dictionary for both the training and test sets\n",
    "    \"\"\"\n",
    "    if fitm:\n",
    "        FitModel = classifier\n",
    "    else:\n",
    "        # Fit model with parameters specified by kwargs\n",
    "        FitModel = classifier(**kwargs).fit(X_train, y_train)\n",
    "\n",
    "    # generate and save predictions on both train and test data\n",
    "    train_pred = FitModel.predict(X_train)\n",
    "    test_pred = FitModel.predict(X_test)\n",
    "    \n",
    "    if len(train_pred) > 1:\n",
    "        # convert back to categorical\n",
    "        train_pred = np.array([train_pred[i].argmax() for i in range(len(train_pred))])\n",
    "        test_pred = np.array([test_pred[i].argmax() for i in range(len(test_pred))])\n",
    "        \n",
    "    # generate and save prediction summary tables for both train and test predictions\n",
    "    pred_counts_train, pred_counts_test = summarize_predictions(\n",
    "        train_pred, test_pred\n",
    "    )\n",
    "    \n",
    "    # generate confusion matrices\n",
    "    conf_matrix_train = make_conf_matrix(train_pred, y_train)\n",
    "    conf_matrix_test = make_conf_matrix(test_pred, y_test)\n",
    "    \n",
    "    # store fitted model, predictions and accuracy scores to dict \n",
    "    model_dict = {\n",
    "        'model': FitModel,\n",
    "        'predictions': {\n",
    "            'train': train_pred,\n",
    "            'test': test_pred,\n",
    "        },\n",
    "        'probabilities': {\n",
    "            'train': FitModel.predict_proba(X_train),\n",
    "            'test': FitModel.predict_proba(X_test),\n",
    "        },\n",
    "        'pred_counts': {\n",
    "            'train': pred_counts_train,\n",
    "            'test': pred_counts_test,\n",
    "        },\n",
    "        'conf_matrix': {\n",
    "            'train': conf_matrix_train,\n",
    "            'test': conf_matrix_test,\n",
    "        },\n",
    "        'class_metrics': {\n",
    "            'train': generate_class_metrics(conf_matrix_train),\n",
    "            'test': generate_class_metrics(conf_matrix_test),\n",
    "        },\n",
    "        'accuracy': {\n",
    "            'train': accuracy_score(y_train, train_pred),\n",
    "            'test': accuracy_score(y_test, test_pred),\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # generate roc and auc metrics for both train and test data if True\n",
    "    if roc_auc:\n",
    "        roc_auc_metrics = ['auc', 'auc_weighted', 'roc_auc_dict']\n",
    "        roc_auc_train = generate_roc_auc(\n",
    "                y_train, model_dict['probabilities']['train'],\n",
    "                class_dict\n",
    "        )\n",
    "        roc_auc_test = generate_roc_auc(\n",
    "                y_test, model_dict['probabilities']['test'],\n",
    "                class_dict\n",
    "        )\n",
    "        # add roc and auc metrics to model_dict        \n",
    "        for name, train_object, test_object in zip(\n",
    "            roc_auc_metrics, roc_auc_train, roc_auc_test\n",
    "        ):\n",
    "            model_dict[name] = {\n",
    "                'train': train_object,\n",
    "                'test': test_object,\n",
    "            }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\t{}\".format(FitModel))\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.452183Z",
     "start_time": "2019-12-11T17:41:54.442400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_save_load_model_dict(model_dict_name, target_directory,\n",
    "                                  classifier, X_train, X_test, y_train, y_test,\n",
    "                                  class_dict, verbose=False, roc_auc=True, overwrite=False,\n",
    "                                  **kwargs):\n",
    "    \"\"\"\n",
    "    Prior to running the generate_model_dict() function, this first\n",
    "    checks to determine whether a saved copy of the specified model has\n",
    "    already been saved to the specified path. \n",
    "    \n",
    "    If no saved dict file is found the function calls generate_model_dict()\n",
    "    and saves the resulting dictionary to the specified filepath.\n",
    "    \n",
    "    If a saved dict file does exist, the function simply loads that file\n",
    "    instead of running the model again.\n",
    "    \n",
    "    NOTE: the resulting target filepath used for saving/checking for the resulting\n",
    "          model_dict is generated as:\n",
    "            \n",
    "            os.path.join(target_directory, ''.join([model_dict_name, '.joblib']))\n",
    "    \n",
    "    model_dict_name: str, specifies the desired name of the model dict object you wish\n",
    "                to generate\n",
    "    target_directory: str, the directory path in which you wish to save or check for\n",
    "                      the resulting model\n",
    "    classifier: the uninitiated sklearn classification model object you wish\n",
    "           to use (e.g. LogisticRegression, KNeighborsClassifier)\n",
    "    X_train, X_test, y_train, y_test: the datasets on which to fit and\n",
    "                                      evaluate the model\n",
    "    class_dict: dict, key values must be the class number (i.e 0, 1, 2, ...) and\n",
    "                the corresponding values must be the class name string (i.e. 'other',\n",
    "                'burlary', ...) for each respective class number\n",
    "    verbose: if True prints resulting fitted model object\n",
    "    roc_auc: if True calculates and stores roc and auc dictionaries for both train\n",
    "             and test\n",
    "    overwrite: bool, default=False, if True, generate_model_dict() will generate a new\n",
    "               dictionary regardless of whether or not the target filepath already\n",
    "               exists and overwrite any file that already exists at the specified\n",
    "               filepath\n",
    "    **kwargs: are optional classifier-specific arguments that pass directly to the model\n",
    "              while fitting\n",
    "\n",
    "    returns: dict, dictionary object containing the resulting fitted model object,\n",
    "             resulting predictions, predicted probabilities, prediction count summary tables,\n",
    "             confusion matrices, accuracy scores, and (if roc_auc=True) the AUC, weighted AUC,\n",
    "             and ROC AUC dictionary for both the training and test sets\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(target_directory, ''.join([model_dict_name, '.joblib']))\n",
    "    \n",
    "    if os.path.exists(filepath) and not overwrite:\n",
    "        locals()[model_dict_name] = joblib.load(filepath)\n",
    "        print(\n",
    "            '\\nThe model dictionary already exists and has been LOADED from:'\\\n",
    "            '\\n\\n\\t{}\\n'.format(filepath)\n",
    "        )\n",
    "    else:\n",
    "        locals()[model_dict_name] = generate_model_dict(\n",
    "            classifier,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            class_dict,\n",
    "            verbose,\n",
    "            roc_auc,\n",
    "            **kwargs\n",
    "        )\n",
    "        dump_loc = joblib.dump(locals()[model_dict_name], filepath)\n",
    "        print(\n",
    "            '\\nThe new model dictionary has been generated and SAVED to disk at:'\\\n",
    "            '\\n\\n\\t{}\\n'.format(dump_loc[0])\n",
    "        )\n",
    "    \n",
    "    return locals()[model_dict_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.471293Z",
     "start_time": "2019-12-11T17:41:54.454055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_model_results(model_dict, accuracy='both', auc='both',\n",
    "                        pred_counts='both', conf_matrix='both',\n",
    "                        class_metrics='both'):\n",
    "    \"\"\"\n",
    "    Prints a model results summary from the model dictionary generated\n",
    "    using the generate_model_dict() function\n",
    "    \n",
    "    model_dict: dict, output dictionary from generate_model_dict() function\n",
    "    accuracy: None, 'both', 'test', or 'train' parameters accepted,\n",
    "              identifies which results to print for this particular metric\n",
    "    auc: same parameters accepted\n",
    "    pred_counts: same parameters accepted\n",
    "    conf_matrix: same parameters accepted\n",
    "    class_metrics: same parameters accepted\n",
    "    \n",
    "    returns: nothing is returned, this function just prints summary output\n",
    "    \"\"\"\n",
    "    train_opt = ['train', 'both']\n",
    "    test_opt = ['test', 'both']\n",
    "    \n",
    "    print('\\nThe fitted model:\\n\\n{}\\n\\n'.format(model_dict['model']))\n",
    "    \n",
    "    if accuracy:\n",
    "        print('This model resulted in the following accuracy:\\n')\n",
    "        if accuracy in train_opt:\n",
    "            print('Training\\t{:.4f}'.format(model_dict['accuracy']['train']))\n",
    "        if accuracy in test_opt:\n",
    "            print('Test\\t\\t{:.4f}'.format(model_dict['accuracy']['test']))\n",
    "        print('\\n')\n",
    "            \n",
    "    if auc:\n",
    "        print('The model AUC is:\\n\\n\\t\\tweighted\\tunweighted')\n",
    "        if auc in train_opt:\n",
    "            print(\n",
    "                'Training\\t{:.4f}\\t\\t{:.4f}'.format(\n",
    "                    model_dict['auc_weighted']['train'],\n",
    "                    model_dict['auc']['train']\n",
    "                )\n",
    "            )\n",
    "        if auc in test_opt:\n",
    "            print(\n",
    "                'Test\\t\\t{:.4f}\\t\\t{:.4f}'.format(\n",
    "                    model_dict['auc_weighted']['test'],\n",
    "                    model_dict['auc']['test']\n",
    "                )\n",
    "            )\n",
    "        print('\\n')\n",
    "        \n",
    "    if pred_counts:\n",
    "        print('The number of classes predicted by class are:\\n')\n",
    "        if pred_counts in train_opt:\n",
    "            print('TRAINING\\n{}\\n'.format(model_dict['pred_counts']['train']))\n",
    "        if pred_counts in test_opt:\n",
    "            print('TEST\\n{}\\n'.format(model_dict['pred_counts']['test']))\n",
    "        print()\n",
    "    \n",
    "    if conf_matrix:\n",
    "        print('The resulting confusion matrix:\\n')\n",
    "        if conf_matrix in train_opt:\n",
    "            print('TRAINING\\n{}\\n'.format(model_dict['conf_matrix']['train']))\n",
    "        if conf_matrix in test_opt:\n",
    "            print('TEST\\n{}\\n'.format(model_dict['conf_matrix']['test']))\n",
    "        print()\n",
    "        \n",
    "    if class_metrics:\n",
    "        print('The classification metrics derived from the confusion matrix are:\\n')\n",
    "        if class_metrics in train_opt:\n",
    "            print('TRAINING\\n{}\\n'.format(model_dict['class_metrics']['train'].iloc[:,:8]))\n",
    "        if class_metrics in test_opt:\n",
    "            print('TEST\\n{}\\n'.format(model_dict['class_metrics']['test'].iloc[:,:8]))\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:41:54.504806Z",
     "start_time": "2019-12-11T17:41:54.475468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# DEFINE FUNCTIONS FOR PLOTTING ROC CURVES\n",
    "##########################################\n",
    "\n",
    "def plot_roc_all_classes(overall_auc, overall_auc_weighted, roc_curve_dict,\n",
    "                         title='ROC plotted for all crime type TEST classes',\n",
    "                         savepath=None):\n",
    "    \"\"\"\n",
    "    Generates a plot of ROC curves for all responses classes\n",
    "    \n",
    "    overall_auc: float, an overall average auc generated using the\n",
    "                 'generate_roc_auc' function\n",
    "    overall_auc_weighted: float, an overall weighted auc generated using the\n",
    "                          'generate_roc_auc' function\n",
    "    roc_curve_dict: dict, an roc_curve dict generated using the 'generate_roc_auc' \n",
    "                    function\n",
    "    title: str, specifies the title used for the plot\n",
    "    savepath: None or str, if none, .png file is NOT saved, otherwise, input the \n",
    "              \"filepath.png\" string, indicating where you would like the image saved\n",
    "    \n",
    "    returns: A plotted image and saved .png file (if savepath is not None)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    plt.title(\n",
    "        ''.join(\n",
    "            [title, '\\n(overall AUC, avg.: {:.3f}; weighted avg.: {:.3})'.format(\n",
    "                overall_auc,\n",
    "                overall_auc_weighted\n",
    "            )\n",
    "            ]\n",
    "        ),\n",
    "        fontsize=20,\n",
    "    )\n",
    "\n",
    "    rate_values=np.arange(0,100)/100\n",
    "\n",
    "    ax.plot(rate_values, rate_values, ':', color='k', linewidth=2, alpha=1)\n",
    "\n",
    "    for key in roc_curve_dict.keys():\n",
    "        plt.plot(\n",
    "            roc_curve_dict[key]['fpr'],\n",
    "            roc_curve_dict[key]['tpr'],\n",
    "            label='{} : {:.3f}'.format(\n",
    "                key,\n",
    "                roc_curve_dict[key]['auc']\n",
    "            )\n",
    "        )\n",
    "        plt.legend(\n",
    "            fontsize=14,\n",
    "            title='class : AUC',\n",
    "            title_fontsize=14,\n",
    "            edgecolor='k',\n",
    "            framealpha=1,\n",
    "            loc=4\n",
    "        )\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.set_ylabel(\"TRUE positive rate\", fontsize=18)\n",
    "    ax.set_xlabel(\"FALSE positive rate\", fontsize=18)\n",
    "    ax.grid(':', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "def plot_roc_all_classes_individual(overall_auc, overall_auc_weighted, roc_curve_dict,\n",
    "                                    title='ROC plotted by crime type TEST class',\n",
    "                                    savepath=None, subplots=(5,2), fig_height=15,\n",
    "                                    suptitle_spacing=0.91):\n",
    "    \"\"\"\n",
    "    Generates a set of subplot of ROC curves for all responses classes, each plotted\n",
    "    individually\n",
    "    \n",
    "    overall_auc: float, an overall average auc generated using the\n",
    "                 'generate_roc_auc' function\n",
    "    overall_auc_weighted: float, an overall weighted auc generated using the\n",
    "                          'generate_roc_auc' function\n",
    "    roc_curve_dict: dict, an roc_curve dict generated using the 'generate_roc_auc' \n",
    "                    function\n",
    "    title: str, specifies the title used for the plot\n",
    "    savepath: None or str, if none, .png file is NOT saved, otherwise, input the \n",
    "              \"filepath.png\" string, indicating where you would like the image saved\n",
    "    subplots: tuple, default=(5,2) to plot each of the 9 crime classes,\n",
    "              provides the dimension of subplots for the figure (NOTE: currently this\n",
    "              function is only configured to plot 2 columns of subplots, therefore\n",
    "              no other value other than two is accepted for the subplots width dimension)\n",
    "    fig_height: int or float, default=15, this value is passed directly to the 'figsize'\n",
    "                parameter of plt.subplots() and determines the overall height of your plot\n",
    "    suptitle_spacing: float between 0 and 1, default=0.91,\n",
    "                      this value is passed to the 'rect' parameter for plt.tight_layout()\n",
    "                      to specify the proportion of the overall plot space to reserve for\n",
    "                      the plt.suptitle() text. If you change the fig_height parameter from\n",
    "                      the default, you may find that you need to adjust suptitle_spacing\n",
    "                      to adjust the position of your suptitle. The larger your fig_height\n",
    "                      value, the larger suptitle_spacing you will need and vice versa.\n",
    "    \n",
    "    returns: A plotted image and saved .png file (if savepath is not None)\n",
    "    \"\"\"\n",
    "    if type(subplots)!=tuple:\n",
    "        raise TypeError(\"'subplots' parameter must be entered as a tuple, e.g. subplots=(3,2)\")\n",
    "    if subplots[1]!=2:\n",
    "        raise ValueError(\n",
    "            \"'subplots' parameter columns dimension must be value=2, e.g. subplots=(5,2). \"\\\n",
    "            \"This function is not configured to handle more or less than 2 subplots.\"\n",
    "        )\n",
    "    \n",
    "    fig, axes = plt.subplots(*subplots, figsize=(12, fig_height))\n",
    "\n",
    "    plt.suptitle(\n",
    "        ''.join(\n",
    "            [title, '\\n(overall AUC, avg.: {:.3f}; weighted avg.: {:.3})'.format(\n",
    "                overall_auc,\n",
    "                overall_auc_weighted\n",
    "            )\n",
    "            ]\n",
    "        ),\n",
    "        fontsize=20,\n",
    "    )\n",
    "\n",
    "    rate_values=np.arange(0,100)/100\n",
    "    \n",
    "    for (i, ax), (j, key) in zip(enumerate(axes.flat), enumerate(roc_curve_dict.keys())):\n",
    "        ax.set_title('class {}: {}'.format(key, roc_curve_dict[key]['name']), fontsize=16)\n",
    "        ax.plot(rate_values, rate_values, ':', color='k', linewidth=2, alpha=.3)\n",
    "\n",
    "        ax.plot(\n",
    "            roc_curve_dict[key]['fpr'],\n",
    "            roc_curve_dict[key]['tpr'],\n",
    "            label='{} ({:.4f})'.format(\n",
    "                key,\n",
    "                roc_curve_dict[key]['auc']\n",
    "            ),\n",
    "            color='k'\n",
    "        )\n",
    "        ax.text(0.7, 0.1, 'AUC = {:.3f}'.format(roc_curve_dict[key]['auc']), fontsize=14)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_ylabel(\"TRUE positive rate\", fontsize=14)\n",
    "        ax.set_xlabel(\"FALSE positive rate\", fontsize=14)\n",
    "        ax.grid(':', alpha=0.4)\n",
    "    \n",
    "    # hide all markings for axes if there is no corresponding subplot\n",
    "    if i < np.product(subplots)-1:\n",
    "        for pos in ['right','top','bottom','left']:\n",
    "            axes[subplots[0]-1, 1].spines[pos].set_visible(False)\n",
    "        axes[subplots[0]-1, 1].tick_params(\n",
    "            axis='x', which='both', bottom=False, top=False, labelbottom=False\n",
    "        )\n",
    "        axes[subplots[0]-1, 1].tick_params(\n",
    "            axis='y', which='both', right=False, left=False, labelleft=False\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, suptitle_spacing])\n",
    "    if savepath:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## First approach with sklearn - DOES NOT WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:42:13.410890Z",
     "start_time": "2019-12-11T17:42:13.345484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Unfortunately this does not seem to work, and I don't know why\n",
    "#  Perhaps the y labels are in an incorrect form (should be one-hot encoded?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:42:31.777079Z",
     "start_time": "2019-12-11T17:42:20.613108Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The fitted model:\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=2, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=20, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "\n",
      "This model resulted in the following accuracy:\n",
      "\n",
      "Training\t0.0492\n",
      "Test\t\t0.0492\n",
      "\n",
      "\n",
      "The model AUC is:\n",
      "\n",
      "\t\tweighted\tunweighted\n",
      "Training\t0.6487\t\t0.6446\n",
      "Test\t\t0.6370\t\t0.6296\n",
      "\n",
      "\n",
      "The number of classes predicted by class are:\n",
      "\n",
      "TRAINING\n",
      "        count  proportion\n",
      "class                    \n",
      "0      128352         1.0\n",
      "\n",
      "TEST\n",
      "       count  proportion\n",
      "class                   \n",
      "0      32088         1.0\n",
      "\n",
      "\n",
      "The resulting confusion matrix:\n",
      "\n",
      "TRAINING\n",
      "Actual        0     1      2     3      4     5      6      7      8   Total\n",
      "Predicted                                                                   \n",
      "0          6321  5664  13082  9587  20767  3423  34555  13710  21243  128352\n",
      "1             0     0      0     0      0     0      0      0      0       0\n",
      "2             0     0      0     0      0     0      0      0      0       0\n",
      "3             0     0      0     0      0     0      0      0      0       0\n",
      "4             0     0      0     0      0     0      0      0      0       0\n",
      "5             0     0      0     0      0     0      0      0      0       0\n",
      "6             0     0      0     0      0     0      0      0      0       0\n",
      "7             0     0      0     0      0     0      0      0      0       0\n",
      "8             0     0      0     0      0     0      0      0      0       0\n",
      "Total      6321  5664  13082  9587  20767  3423  34555  13710  21243  128352\n",
      "\n",
      "TEST\n",
      "Actual        0     1     2     3     4    5     6     7     8  Total\n",
      "Predicted                                                            \n",
      "0          1580  1416  3271  2397  5192  856  8639  3427  5310  32088\n",
      "1             0     0     0     0     0    0     0     0     0      0\n",
      "2             0     0     0     0     0    0     0     0     0      0\n",
      "3             0     0     0     0     0    0     0     0     0      0\n",
      "4             0     0     0     0     0    0     0     0     0      0\n",
      "5             0     0     0     0     0    0     0     0     0      0\n",
      "6             0     0     0     0     0    0     0     0     0      0\n",
      "7             0     0     0     0     0    0     0     0     0      0\n",
      "8             0     0     0     0     0    0     0     0     0      0\n",
      "Total      1580  1416  3271  2397  5192  856  8639  3427  5310  32088\n",
      "\n",
      "\n",
      "The classification metrics derived from the confusion matrix are:\n",
      "\n",
      "TRAINING\n",
      "         TP      FP     FN      TN  TPR  FNR  FPR  TNR\n",
      "class                                                 \n",
      "0      6321  122031      0       0  1.0  0.0  1.0  0.0\n",
      "1         0       0   5664  122688  0.0  1.0  0.0  1.0\n",
      "2         0       0  13082  115270  0.0  1.0  0.0  1.0\n",
      "3         0       0   9587  118765  0.0  1.0  0.0  1.0\n",
      "4         0       0  20767  107585  0.0  1.0  0.0  1.0\n",
      "5         0       0   3423  124929  0.0  1.0  0.0  1.0\n",
      "6         0       0  34555   93797  0.0  1.0  0.0  1.0\n",
      "7         0       0  13710  114642  0.0  1.0  0.0  1.0\n",
      "8         0       0  21243  107109  0.0  1.0  0.0  1.0\n",
      "\n",
      "TEST\n",
      "         TP     FP    FN     TN  TPR  FNR  FPR  TNR\n",
      "class                                              \n",
      "0      1580  30508     0      0  1.0  0.0  1.0  0.0\n",
      "1         0      0  1416  30672  0.0  1.0  0.0  1.0\n",
      "2         0      0  3271  28817  0.0  1.0  0.0  1.0\n",
      "3         0      0  2397  29691  0.0  1.0  0.0  1.0\n",
      "4         0      0  5192  26896  0.0  1.0  0.0  1.0\n",
      "5         0      0   856  31232  0.0  1.0  0.0  1.0\n",
      "6         0      0  8639  23449  0.0  1.0  0.0  1.0\n",
      "7         0      0  3427  28661  0.0  1.0  0.0  1.0\n",
      "8         0      0  5310  26778  0.0  1.0  0.0  1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnlogloc_dict = generate_model_dict(\n",
    "    MLPClassifier,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    class_dict,\n",
    "    max_iter=2,\n",
    "    hidden_layer_sizes=(100, 100, 100,),\n",
    "    random_state=20\n",
    ")\n",
    "\n",
    "print_model_results(mnlogloc_dict,accuracy='both', auc='both',\n",
    "                        pred_counts='both', conf_matrix='both',\n",
    "                        class_metrics='both')\n",
    "\n",
    "#auc_inputs = [mnlogloc_dict[key]['test'] for key in ['auc', 'auc_weighted', 'roc_auc_dict']]\n",
    "\n",
    "#title = 'ROC plotted for all crime type TEST classes, logistic regression\\n'\\\n",
    "#        'with only latitude and longitude as predictors'\n",
    "#savepath = os.path.join(FIGURES_ROOT, 'roc-lat-lon-only.png')\n",
    "\n",
    "#plot_roc_all_classes(*auc_inputs, title, savepath)\n",
    "\n",
    "#savepath = os.path.join(FIGURES_ROOT, 'roc-by-class-lat-lon-only.png')\n",
    "\n",
    "#plot_roc_all_classes_individual(*auc_inputs, title, savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - getting a base model to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:43:27.887997Z",
     "start_time": "2019-12-11T17:43:27.883296Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:43:29.009294Z",
     "start_time": "2019-12-11T17:43:28.812708Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:43:31.612860Z",
     "start_time": "2019-12-11T17:43:29.373312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier(n_estimators=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:43:33.002436Z",
     "start_time": "2019-12-11T17:43:32.851180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3086200448765894"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmark to beat\n",
    "_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson number 1: for unbalanced datasets, use class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:44:01.983356Z",
     "start_time": "2019-12-11T17:44:01.980328Z"
    }
   },
   "outputs": [],
   "source": [
    "# We weight different classes differently to account for inbalances in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:44:04.555892Z",
     "start_time": "2019-12-11T17:44:04.509021Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights =             compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:44:44.412553Z",
     "start_time": "2019-12-11T17:44:44.271885Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=43),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(9, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:44:48.474515Z",
     "start_time": "2019-12-11T17:44:48.464414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 64)                2816      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 11,721\n",
      "Trainable params: 11,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to scale your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T23:43:30.888201Z",
     "start_time": "2019-12-08T23:40:05.533231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128352/128352 [==============================] - 19s 147us/step - loss: 1.9612 - sparse_categorical_accuracy: 0.2837\n",
      "Epoch 2/10\n",
      "128352/128352 [==============================] - 19s 150us/step - loss: 1.9271 - sparse_categorical_accuracy: 0.2943\n",
      "Epoch 3/10\n",
      "128352/128352 [==============================] - 20s 160us/step - loss: 1.9145 - sparse_categorical_accuracy: 0.2978\n",
      "Epoch 4/10\n",
      "128352/128352 [==============================] - 20s 156us/step - loss: 1.9067 - sparse_categorical_accuracy: 0.3019\n",
      "Epoch 5/10\n",
      "128352/128352 [==============================] - 20s 157us/step - loss: 1.9011 - sparse_categorical_accuracy: 0.3041\n",
      "Epoch 6/10\n",
      "128352/128352 [==============================] - 29s 229us/step - loss: 1.8968 - sparse_categorical_accuracy: 0.3058\n",
      "Epoch 7/10\n",
      "128352/128352 [==============================] - 22s 173us/step - loss: 1.8932 - sparse_categorical_accuracy: 0.3073\n",
      "Epoch 8/10\n",
      "128352/128352 [==============================] - 18s 142us/step - loss: 1.8901 - sparse_categorical_accuracy: 0.3086\n",
      "Epoch 9/10\n",
      "128352/128352 [==============================] - 19s 144us/step - loss: 1.8872 - sparse_categorical_accuracy: 0.3107\n",
      "Epoch 10/10\n",
      "128352/128352 [==============================] - 18s 142us/step - loss: 1.8846 - sparse_categorical_accuracy: 0.3122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4b97bac8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T23:46:04.045325Z",
     "start_time": "2019-12-08T23:46:01.644553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32088/32088 [==============================] - 2s 74us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8954089697505196, 0.30662552980049923]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T23:45:45.709857Z",
     "start_time": "2019-12-08T23:45:30.217412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The fitted model:\n",
      "\n",
      "<keras.engine.sequential.Sequential object at 0x1c4b382550>\n",
      "\n",
      "\n",
      "This model resulted in the following accuracy:\n",
      "\n",
      "Training\t1.0000\n",
      "Test\t\t1.0000\n",
      "\n",
      "\n",
      "The model AUC is:\n",
      "\n",
      "\t\tweighted\tunweighted\n",
      "Training\t0.6482\t\t0.6427\n",
      "Test\t\t0.4981\t\t0.4995\n",
      "\n",
      "\n",
      "The number of classes predicted by class are:\n",
      "\n",
      "TRAINING\n",
      "       count  proportion\n",
      "class                   \n",
      "0       6321    0.049247\n",
      "1       5664    0.044129\n",
      "2      13082    0.101923\n",
      "3       9587    0.074693\n",
      "4      20767    0.161797\n",
      "5       3423    0.026669\n",
      "6      34555    0.269221\n",
      "7      13710    0.106816\n",
      "8      21243    0.165506\n",
      "\n",
      "TEST\n",
      "       count  proportion\n",
      "class                   \n",
      "0       1638    0.051047\n",
      "1       1437    0.044783\n",
      "2       3199    0.099695\n",
      "3       2304    0.071803\n",
      "4       5206    0.162241\n",
      "5        823    0.025648\n",
      "6       8696    0.271005\n",
      "7       3369    0.104993\n",
      "8       5416    0.168786\n",
      "\n",
      "\n",
      "The resulting confusion matrix:\n",
      "\n",
      "TRAINING\n",
      "Actual        0     1      2     3      4     5      6      7      8   Total\n",
      "Predicted                                                                   \n",
      "0          6321     0      0     0      0     0      0      0      0    6321\n",
      "1             0  5664      0     0      0     0      0      0      0    5664\n",
      "2             0     0  13082     0      0     0      0      0      0   13082\n",
      "3             0     0      0  9587      0     0      0      0      0    9587\n",
      "4             0     0      0     0  20767     0      0      0      0   20767\n",
      "5             0     0      0     0      0  3423      0      0      0    3423\n",
      "6             0     0      0     0      0     0  34555      0      0   34555\n",
      "7             0     0      0     0      0     0      0  13710      0   13710\n",
      "8             0     0      0     0      0     0      0      0  21243   21243\n",
      "Total      6321  5664  13082  9587  20767  3423  34555  13710  21243  128352\n",
      "\n",
      "TEST\n",
      "Actual        0     1     2     3     4    5     6     7     8  Total\n",
      "Predicted                                                            \n",
      "0          1638     0     0     0     0    0     0     0     0   1638\n",
      "1             0  1437     0     0     0    0     0     0     0   1437\n",
      "2             0     0  3199     0     0    0     0     0     0   3199\n",
      "3             0     0     0  2304     0    0     0     0     0   2304\n",
      "4             0     0     0     0  5206    0     0     0     0   5206\n",
      "5             0     0     0     0     0  823     0     0     0    823\n",
      "6             0     0     0     0     0    0  8696     0     0   8696\n",
      "7             0     0     0     0     0    0     0  3369     0   3369\n",
      "8             0     0     0     0     0    0     0     0  5416   5416\n",
      "Total      1638  1437  3199  2304  5206  823  8696  3369  5416  32088\n",
      "\n",
      "\n",
      "The classification metrics derived from the confusion matrix are:\n",
      "\n",
      "TRAINING\n",
      "          TP  FP  FN      TN  TPR  FNR  FPR  TNR\n",
      "class                                           \n",
      "0       6321   0   0  122031  1.0  0.0  0.0  1.0\n",
      "1       5664   0   0  122688  1.0  0.0  0.0  1.0\n",
      "2      13082   0   0  115270  1.0  0.0  0.0  1.0\n",
      "3       9587   0   0  118765  1.0  0.0  0.0  1.0\n",
      "4      20767   0   0  107585  1.0  0.0  0.0  1.0\n",
      "5       3423   0   0  124929  1.0  0.0  0.0  1.0\n",
      "6      34555   0   0   93797  1.0  0.0  0.0  1.0\n",
      "7      13710   0   0  114642  1.0  0.0  0.0  1.0\n",
      "8      21243   0   0  107109  1.0  0.0  0.0  1.0\n",
      "\n",
      "TEST\n",
      "         TP  FP  FN     TN  TPR  FNR  FPR  TNR\n",
      "class                                         \n",
      "0      1638   0   0  30450  1.0  0.0  0.0  1.0\n",
      "1      1437   0   0  30651  1.0  0.0  0.0  1.0\n",
      "2      3199   0   0  28889  1.0  0.0  0.0  1.0\n",
      "3      2304   0   0  29784  1.0  0.0  0.0  1.0\n",
      "4      5206   0   0  26882  1.0  0.0  0.0  1.0\n",
      "5       823   0   0  31265  1.0  0.0  0.0  1.0\n",
      "6      8696   0   0  23392  1.0  0.0  0.0  1.0\n",
      "7      3369   0   0  28719  1.0  0.0  0.0  1.0\n",
      "8      5416   0   0  26672  1.0  0.0  0.0  1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnlogloc_dict = generate_model_dict(\n",
    "    model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    class_dict,\n",
    "    max_iter=100,\n",
    "    hidden_layer_sizes=(100, 100, 100,),\n",
    "    random_state=20,\n",
    "    fitm=True #do not re-fit\n",
    ")\n",
    "\n",
    "print_model_results(mnlogloc_dict,accuracy='both', auc='both',\n",
    "                        pred_counts='both', conf_matrix='both',\n",
    "                        class_metrics='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First working model, auc: 0.645, acc: 0.315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:54:04.171078Z",
     "start_time": "2019-12-11T17:54:03.975247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, input_dim=43),\n",
    "    LeakyReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256),\n",
    "    LeakyReLU(),\n",
    "    Dense(256),\n",
    "    LeakyReLU(),\n",
    "    Dense(256),\n",
    "    LeakyReLU(),\n",
    "    Dense(9, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T17:54:09.875727Z",
     "start_time": "2019-12-11T17:54:09.866989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 256)               11264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 210,953\n",
      "Trainable params: 210,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:00:38.721146Z",
     "start_time": "2019-12-09T08:51:31.267579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 115516 samples, validate on 12836 samples\n",
      "Epoch 1/50\n",
      "115516/115516 [==============================] - 69s 597us/step - loss: 1.9440 - sparse_categorical_accuracy: 0.2882 - val_loss: 1.9178 - val_sparse_categorical_accuracy: 0.2964\n",
      "Epoch 2/50\n",
      "115516/115516 [==============================] - 53s 458us/step - loss: 1.9238 - sparse_categorical_accuracy: 0.2954 - val_loss: 1.9015 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 3/50\n",
      "115516/115516 [==============================] - 51s 443us/step - loss: 1.9174 - sparse_categorical_accuracy: 0.2996 - val_loss: 1.8996 - val_sparse_categorical_accuracy: 0.3004\n",
      "Epoch 4/50\n",
      "115516/115516 [==============================] - 54s 469us/step - loss: 1.9124 - sparse_categorical_accuracy: 0.2998 - val_loss: 1.9033 - val_sparse_categorical_accuracy: 0.2992\n",
      "Epoch 5/50\n",
      "115516/115516 [==============================] - 52s 449us/step - loss: 1.9092 - sparse_categorical_accuracy: 0.3021 - val_loss: 1.8903 - val_sparse_categorical_accuracy: 0.3069\n",
      "Epoch 6/50\n",
      "115516/115516 [==============================] - 52s 453us/step - loss: 1.9068 - sparse_categorical_accuracy: 0.3030 - val_loss: 1.8873 - val_sparse_categorical_accuracy: 0.3069\n",
      "Epoch 7/50\n",
      "115516/115516 [==============================] - 52s 454us/step - loss: 1.9053 - sparse_categorical_accuracy: 0.3035 - val_loss: 1.8868 - val_sparse_categorical_accuracy: 0.3000\n",
      "Epoch 8/50\n",
      "115516/115516 [==============================] - 54s 471us/step - loss: 1.9023 - sparse_categorical_accuracy: 0.3051 - val_loss: 1.8839 - val_sparse_categorical_accuracy: 0.3101\n",
      "Epoch 9/50\n",
      "115516/115516 [==============================] - 52s 447us/step - loss: 1.9008 - sparse_categorical_accuracy: 0.3066 - val_loss: 1.8878 - val_sparse_categorical_accuracy: 0.3018\n",
      "Epoch 10/50\n",
      "115516/115516 [==============================] - 56s 487us/step - loss: 1.8995 - sparse_categorical_accuracy: 0.3068 - val_loss: 1.8837 - val_sparse_categorical_accuracy: 0.3086\n",
      "Epoch 11/50\n",
      "   736/115516 [..............................] - ETA: 1:47 - loss: 1.8835 - sparse_categorical_accuracy: 0.2976"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e5a2744794de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, class_weight=class_weights, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:00:44.308145Z",
     "start_time": "2019-12-09T09:00:43.593896Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('basic-nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:01:05.261632Z",
     "start_time": "2019-12-09T09:01:02.492370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32088/32088 [==============================] - 3s 86us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8952100647265582, 0.3070306656768393]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T10:41:41.209683Z",
     "start_time": "2019-12-09T10:41:17.561301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The fitted model:\n",
      "\n",
      "<keras.engine.sequential.Sequential object at 0x1c51c84048>\n",
      "\n",
      "\n",
      "This model resulted in the following accuracy:\n",
      "\n",
      "Training\t0.3108\n",
      "Test\t\t0.3070\n",
      "\n",
      "\n",
      "The model AUC is:\n",
      "\n",
      "\t\tweighted\tunweighted\n",
      "Training\t0.6459\t\t0.6415\n",
      "Test\t\t0.6377\t\t0.6315\n",
      "\n",
      "\n",
      "The number of classes predicted by class are:\n",
      "\n",
      "TRAINING\n",
      "       count  proportion\n",
      "class                   \n",
      "0       1473    0.011476\n",
      "1          5    0.000039\n",
      "2       7095    0.055278\n",
      "4      49613    0.386539\n",
      "6      66897    0.521200\n",
      "7         16    0.000125\n",
      "8       3253    0.025344\n",
      "\n",
      "TEST\n",
      "       count  proportion\n",
      "class                   \n",
      "0        339    0.010565\n",
      "1          2    0.000062\n",
      "2       1744    0.054351\n",
      "4      12364    0.385315\n",
      "6      16826    0.524370\n",
      "7          2    0.000062\n",
      "8        811    0.025274\n",
      "\n",
      "\n",
      "The resulting confusion matrix:\n",
      "\n",
      "TRAINING\n",
      "Actual        0     1      2     3      4     5      6      7      8   Total\n",
      "Predicted                                                                   \n",
      "0           406   136     30    80    158    28    309    129    197    1473\n",
      "1             0     2      0     1      1     0      1      0      0       5\n",
      "2           656   129   2715   329    754   167    949    414    982    7095\n",
      "3             0     0      0     0      0     0      0      0      0       0\n",
      "4          1867  2162   3774  3310  12329  1407   9328   6095   9341   49613\n",
      "5             0     0      0     0      0     0      0      0      0       0\n",
      "6          3090  3152   6335  5710   7150  1657  23402   6713   9688   66897\n",
      "7             0     0      0     1      3     0      4      7      1      16\n",
      "8           302    83    228   156    372   164    562    352   1034    3253\n",
      "Total      6321  5664  13082  9587  20767  3423  34555  13710  21243  128352\n",
      "\n",
      "TEST\n",
      "Actual        0     1     2     3     4    5     6     7     8  Total\n",
      "Predicted                                                            \n",
      "0            87    21     8    17    43    4    66    41    52    339\n",
      "1             0     0     0     1     0    0     1     0     0      2\n",
      "2           130    22   651    78   205   44   247   117   250   1744\n",
      "3             0     0     0     0     0    0     0     0     0      0\n",
      "4           514   543   942   799  3081  335  2386  1457  2307  12364\n",
      "5             0     0     0     0     0    0     0     0     0      0\n",
      "6           786   804  1614  1467  1755  417  5793  1729  2461  16826\n",
      "7             0     1     0     0     1    0     0     0     0      2\n",
      "8            63    25    56    35   107   56   146    83   240    811\n",
      "Total      1580  1416  3271  2397  5192  856  8639  3427  5310  32088\n",
      "\n",
      "\n",
      "The classification metrics derived from the confusion matrix are:\n",
      "\n",
      "TRAINING\n",
      "          TP     FP     FN      TN       TPR       FNR       FPR       TNR\n",
      "class                                                                     \n",
      "0        406   1067   5915  120964  0.064230  0.935770  0.008744  0.991256\n",
      "1          2      3   5662  122685  0.000353  0.999647  0.000024  0.999976\n",
      "2       2715   4380  10367  110890  0.207537  0.792463  0.037998  0.962002\n",
      "3          0      0   9587  118765  0.000000  1.000000  0.000000  1.000000\n",
      "4      12329  37284   8438   70301  0.593682  0.406318  0.346554  0.653446\n",
      "5          0      0   3423  124929  0.000000  1.000000  0.000000  1.000000\n",
      "6      23402  43495  11153   50302  0.677239  0.322761  0.463714  0.536286\n",
      "7          7      9  13703  114633  0.000511  0.999489  0.000079  0.999921\n",
      "8       1034   2219  20209  104890  0.048675  0.951325  0.020717  0.979283\n",
      "\n",
      "TEST\n",
      "         TP     FP    FN     TN       TPR       FNR       FPR       TNR\n",
      "class                                                                  \n",
      "0        87    252  1493  30256  0.055063  0.944937  0.008260  0.991740\n",
      "1         0      2  1416  30670  0.000000  1.000000  0.000065  0.999935\n",
      "2       651   1093  2620  27724  0.199022  0.800978  0.037929  0.962071\n",
      "3         0      0  2397  29691  0.000000  1.000000  0.000000  1.000000\n",
      "4      3081   9283  2111  17613  0.593413  0.406587  0.345144  0.654856\n",
      "5         0      0   856  31232  0.000000  1.000000  0.000000  1.000000\n",
      "6      5793  11033  2846  12416  0.670564  0.329436  0.470510  0.529490\n",
      "7         0      2  3427  28659  0.000000  1.000000  0.000070  0.999930\n",
      "8       240    571  5070  26207  0.045198  0.954802  0.021323  0.978677\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_dict = generate_model_dict(\n",
    "    model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    class_dict,\n",
    "    random_state=20,\n",
    "    fitm=True #do not re-fit\n",
    ")\n",
    "\n",
    "print_model_results(nn_dict,accuracy='both', auc='both',\n",
    "                        pred_counts='both', conf_matrix='both',\n",
    "                        class_metrics='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:17:23.172152Z",
     "start_time": "2019-12-09T00:17:23.168634Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T00:52:25.078011Z",
     "start_time": "2019-12-09T00:52:24.808096Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "pars = {'l1': 0.000001, 'l2': 0.00001}\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, input_dim=43, kernel_regularizer=l1_l2(**pars), activity_regularizer=l1_l2(**pars)),\n",
    "    LeakyReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, kernel_regularizer=l1_l2(**pars), activity_regularizer=l1_l2(**pars)),\n",
    "    LeakyReLU(),\n",
    "    Dense(256, kernel_regularizer=l1_l2(**pars), activity_regularizer=l1_l2(**pars)),\n",
    "    LeakyReLU(),\n",
    "    Dense(9, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='nadam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy,\n",
    "                       tf.keras.metrics.sparse_top_k_categorical_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T02:19:27.697619Z",
     "start_time": "2019-12-09T00:52:25.080262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102681 samples, validate on 25671 samples\n",
      "Epoch 1/100\n",
      "102681/102681 [==============================] - 69s 668us/step - loss: 1.9823 - sparse_categorical_accuracy: 0.2864 - sparse_top_k_categorical_accuracy: 0.8164 - val_loss: 1.9504 - val_sparse_categorical_accuracy: 0.2937 - val_sparse_top_k_categorical_accuracy: 0.8248\n",
      "Epoch 2/100\n",
      "102681/102681 [==============================] - 62s 608us/step - loss: 1.9595 - sparse_categorical_accuracy: 0.2911 - sparse_top_k_categorical_accuracy: 0.8220 - val_loss: 1.9485 - val_sparse_categorical_accuracy: 0.2925 - val_sparse_top_k_categorical_accuracy: 0.8245\n",
      "Epoch 3/100\n",
      "102681/102681 [==============================] - 63s 618us/step - loss: 1.9551 - sparse_categorical_accuracy: 0.2933 - sparse_top_k_categorical_accuracy: 0.8236 - val_loss: 1.9432 - val_sparse_categorical_accuracy: 0.2978 - val_sparse_top_k_categorical_accuracy: 0.8237\n",
      "Epoch 4/100\n",
      "102681/102681 [==============================] - 63s 616us/step - loss: 1.9533 - sparse_categorical_accuracy: 0.2933 - sparse_top_k_categorical_accuracy: 0.8234 - val_loss: 1.9393 - val_sparse_categorical_accuracy: 0.3024 - val_sparse_top_k_categorical_accuracy: 0.8251\n",
      "Epoch 5/100\n",
      "102681/102681 [==============================] - 63s 614us/step - loss: 1.9522 - sparse_categorical_accuracy: 0.2941 - sparse_top_k_categorical_accuracy: 0.8234 - val_loss: 1.9413 - val_sparse_categorical_accuracy: 0.2909 - val_sparse_top_k_categorical_accuracy: 0.8277\n",
      "Epoch 6/100\n",
      "102681/102681 [==============================] - 63s 618us/step - loss: 1.9515 - sparse_categorical_accuracy: 0.2947 - sparse_top_k_categorical_accuracy: 0.8247 - val_loss: 1.9389 - val_sparse_categorical_accuracy: 0.3033 - val_sparse_top_k_categorical_accuracy: 0.8253\n",
      "Epoch 7/100\n",
      "102681/102681 [==============================] - 63s 618us/step - loss: 1.9512 - sparse_categorical_accuracy: 0.2953 - sparse_top_k_categorical_accuracy: 0.8239 - val_loss: 1.9436 - val_sparse_categorical_accuracy: 0.3001 - val_sparse_top_k_categorical_accuracy: 0.8254\n",
      "Epoch 8/100\n",
      "102681/102681 [==============================] - 65s 634us/step - loss: 1.9510 - sparse_categorical_accuracy: 0.2963 - sparse_top_k_categorical_accuracy: 0.8241 - val_loss: 1.9391 - val_sparse_categorical_accuracy: 0.3009 - val_sparse_top_k_categorical_accuracy: 0.8284\n",
      "Epoch 9/100\n",
      "102681/102681 [==============================] - 66s 642us/step - loss: 1.9498 - sparse_categorical_accuracy: 0.2967 - sparse_top_k_categorical_accuracy: 0.8253 - val_loss: 1.9414 - val_sparse_categorical_accuracy: 0.2990 - val_sparse_top_k_categorical_accuracy: 0.8264\n",
      "Epoch 10/100\n",
      "102681/102681 [==============================] - 64s 624us/step - loss: 1.9500 - sparse_categorical_accuracy: 0.2966 - sparse_top_k_categorical_accuracy: 0.8248 - val_loss: 1.9393 - val_sparse_categorical_accuracy: 0.2986 - val_sparse_top_k_categorical_accuracy: 0.8245\n",
      "Epoch 11/100\n",
      "102681/102681 [==============================] - 64s 623us/step - loss: 1.9492 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8246 - val_loss: 1.9411 - val_sparse_categorical_accuracy: 0.3040 - val_sparse_top_k_categorical_accuracy: 0.8259\n",
      "Epoch 12/100\n",
      "102681/102681 [==============================] - 64s 622us/step - loss: 1.9492 - sparse_categorical_accuracy: 0.2957 - sparse_top_k_categorical_accuracy: 0.8252 - val_loss: 1.9324 - val_sparse_categorical_accuracy: 0.3042 - val_sparse_top_k_categorical_accuracy: 0.8291\n",
      "Epoch 13/100\n",
      "102681/102681 [==============================] - 64s 627us/step - loss: 1.9479 - sparse_categorical_accuracy: 0.2970 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9348 - val_sparse_categorical_accuracy: 0.3015 - val_sparse_top_k_categorical_accuracy: 0.8279\n",
      "Epoch 14/100\n",
      "102681/102681 [==============================] - 64s 622us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9413 - val_sparse_categorical_accuracy: 0.3045 - val_sparse_top_k_categorical_accuracy: 0.8260\n",
      "Epoch 15/100\n",
      "102681/102681 [==============================] - 62s 603us/step - loss: 1.9489 - sparse_categorical_accuracy: 0.2975 - sparse_top_k_categorical_accuracy: 0.8247 - val_loss: 1.9390 - val_sparse_categorical_accuracy: 0.3010 - val_sparse_top_k_categorical_accuracy: 0.8240\n",
      "Epoch 16/100\n",
      "102681/102681 [==============================] - 51s 493us/step - loss: 1.9484 - sparse_categorical_accuracy: 0.2968 - sparse_top_k_categorical_accuracy: 0.8253 - val_loss: 1.9420 - val_sparse_categorical_accuracy: 0.2979 - val_sparse_top_k_categorical_accuracy: 0.8266\n",
      "Epoch 17/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9482 - sparse_categorical_accuracy: 0.2978 - sparse_top_k_categorical_accuracy: 0.8257 - val_loss: 1.9365 - val_sparse_categorical_accuracy: 0.3025 - val_sparse_top_k_categorical_accuracy: 0.8267\n",
      "Epoch 18/100\n",
      "102681/102681 [==============================] - 51s 495us/step - loss: 1.9486 - sparse_categorical_accuracy: 0.2979 - sparse_top_k_categorical_accuracy: 0.8250 - val_loss: 1.9334 - val_sparse_categorical_accuracy: 0.3009 - val_sparse_top_k_categorical_accuracy: 0.8289\n",
      "Epoch 19/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9483 - sparse_categorical_accuracy: 0.2984 - sparse_top_k_categorical_accuracy: 0.8265 - val_loss: 1.9394 - val_sparse_categorical_accuracy: 0.3027 - val_sparse_top_k_categorical_accuracy: 0.8254\n",
      "Epoch 20/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9486 - sparse_categorical_accuracy: 0.2991 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9349 - val_sparse_categorical_accuracy: 0.3021 - val_sparse_top_k_categorical_accuracy: 0.8298\n",
      "Epoch 21/100\n",
      "102681/102681 [==============================] - 50s 489us/step - loss: 1.9487 - sparse_categorical_accuracy: 0.2971 - sparse_top_k_categorical_accuracy: 0.8252 - val_loss: 1.9348 - val_sparse_categorical_accuracy: 0.3034 - val_sparse_top_k_categorical_accuracy: 0.8280\n",
      "Epoch 22/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9480 - sparse_categorical_accuracy: 0.2970 - sparse_top_k_categorical_accuracy: 0.8255 - val_loss: 1.9404 - val_sparse_categorical_accuracy: 0.2992 - val_sparse_top_k_categorical_accuracy: 0.8247\n",
      "Epoch 23/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9347 - val_sparse_categorical_accuracy: 0.3029 - val_sparse_top_k_categorical_accuracy: 0.8281\n",
      "Epoch 24/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9480 - sparse_categorical_accuracy: 0.2978 - sparse_top_k_categorical_accuracy: 0.8250 - val_loss: 1.9345 - val_sparse_categorical_accuracy: 0.3018 - val_sparse_top_k_categorical_accuracy: 0.8269\n",
      "Epoch 25/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9478 - sparse_categorical_accuracy: 0.2969 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9317 - val_sparse_categorical_accuracy: 0.3051 - val_sparse_top_k_categorical_accuracy: 0.8287\n",
      "Epoch 26/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9482 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8263 - val_loss: 1.9384 - val_sparse_categorical_accuracy: 0.3026 - val_sparse_top_k_categorical_accuracy: 0.8266\n",
      "Epoch 27/100\n",
      "102681/102681 [==============================] - 50s 489us/step - loss: 1.9474 - sparse_categorical_accuracy: 0.2975 - sparse_top_k_categorical_accuracy: 0.8251 - val_loss: 1.9322 - val_sparse_categorical_accuracy: 0.3032 - val_sparse_top_k_categorical_accuracy: 0.8298\n",
      "Epoch 28/100\n",
      "102681/102681 [==============================] - 51s 496us/step - loss: 1.9483 - sparse_categorical_accuracy: 0.2968 - sparse_top_k_categorical_accuracy: 0.8250 - val_loss: 1.9341 - val_sparse_categorical_accuracy: 0.3023 - val_sparse_top_k_categorical_accuracy: 0.8260\n",
      "Epoch 29/100\n",
      "102681/102681 [==============================] - 51s 499us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2971 - sparse_top_k_categorical_accuracy: 0.8255 - val_loss: 1.9395 - val_sparse_categorical_accuracy: 0.2998 - val_sparse_top_k_categorical_accuracy: 0.8277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9475 - sparse_categorical_accuracy: 0.2976 - sparse_top_k_categorical_accuracy: 0.8251 - val_loss: 1.9373 - val_sparse_categorical_accuracy: 0.3017 - val_sparse_top_k_categorical_accuracy: 0.8271\n",
      "Epoch 31/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2973 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9407 - val_sparse_categorical_accuracy: 0.2988 - val_sparse_top_k_categorical_accuracy: 0.8270\n",
      "Epoch 32/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9483 - sparse_categorical_accuracy: 0.2967 - sparse_top_k_categorical_accuracy: 0.8253 - val_loss: 1.9389 - val_sparse_categorical_accuracy: 0.3031 - val_sparse_top_k_categorical_accuracy: 0.8266\n",
      "Epoch 33/100\n",
      "102681/102681 [==============================] - 50s 482us/step - loss: 1.9477 - sparse_categorical_accuracy: 0.2970 - sparse_top_k_categorical_accuracy: 0.8257 - val_loss: 1.9429 - val_sparse_categorical_accuracy: 0.2980 - val_sparse_top_k_categorical_accuracy: 0.8275\n",
      "Epoch 34/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8265 - val_loss: 1.9335 - val_sparse_categorical_accuracy: 0.3062 - val_sparse_top_k_categorical_accuracy: 0.8282\n",
      "Epoch 35/100\n",
      "102681/102681 [==============================] - 50s 490us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2981 - sparse_top_k_categorical_accuracy: 0.8255 - val_loss: 1.9350 - val_sparse_categorical_accuracy: 0.3010 - val_sparse_top_k_categorical_accuracy: 0.8276\n",
      "Epoch 36/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9470 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8260 - val_loss: 1.9359 - val_sparse_categorical_accuracy: 0.2988 - val_sparse_top_k_categorical_accuracy: 0.8273\n",
      "Epoch 37/100\n",
      "102681/102681 [==============================] - 50s 489us/step - loss: 1.9486 - sparse_categorical_accuracy: 0.2964 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9337 - val_sparse_categorical_accuracy: 0.3017 - val_sparse_top_k_categorical_accuracy: 0.8284\n",
      "Epoch 38/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9475 - sparse_categorical_accuracy: 0.2979 - sparse_top_k_categorical_accuracy: 0.8262 - val_loss: 1.9349 - val_sparse_categorical_accuracy: 0.3001 - val_sparse_top_k_categorical_accuracy: 0.8281\n",
      "Epoch 39/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2982 - sparse_top_k_categorical_accuracy: 0.8254 - val_loss: 1.9331 - val_sparse_categorical_accuracy: 0.3001 - val_sparse_top_k_categorical_accuracy: 0.8314\n",
      "Epoch 40/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9485 - sparse_categorical_accuracy: 0.2976 - sparse_top_k_categorical_accuracy: 0.8251 - val_loss: 1.9413 - val_sparse_categorical_accuracy: 0.2944 - val_sparse_top_k_categorical_accuracy: 0.8281\n",
      "Epoch 41/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9477 - sparse_categorical_accuracy: 0.2979 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9314 - val_sparse_categorical_accuracy: 0.3019 - val_sparse_top_k_categorical_accuracy: 0.8292\n",
      "Epoch 42/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9468 - sparse_categorical_accuracy: 0.2974 - sparse_top_k_categorical_accuracy: 0.8269 - val_loss: 1.9389 - val_sparse_categorical_accuracy: 0.2989 - val_sparse_top_k_categorical_accuracy: 0.8265\n",
      "Epoch 43/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9482 - sparse_categorical_accuracy: 0.2966 - sparse_top_k_categorical_accuracy: 0.8260 - val_loss: 1.9402 - val_sparse_categorical_accuracy: 0.3004 - val_sparse_top_k_categorical_accuracy: 0.8274\n",
      "Epoch 44/100\n",
      "102681/102681 [==============================] - 51s 494us/step - loss: 1.9475 - sparse_categorical_accuracy: 0.2975 - sparse_top_k_categorical_accuracy: 0.8265 - val_loss: 1.9330 - val_sparse_categorical_accuracy: 0.3029 - val_sparse_top_k_categorical_accuracy: 0.8295\n",
      "Epoch 45/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9478 - sparse_categorical_accuracy: 0.2975 - sparse_top_k_categorical_accuracy: 0.8265 - val_loss: 1.9389 - val_sparse_categorical_accuracy: 0.2995 - val_sparse_top_k_categorical_accuracy: 0.8277\n",
      "Epoch 46/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9480 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9363 - val_sparse_categorical_accuracy: 0.3025 - val_sparse_top_k_categorical_accuracy: 0.8277\n",
      "Epoch 47/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9481 - sparse_categorical_accuracy: 0.2960 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9363 - val_sparse_categorical_accuracy: 0.2987 - val_sparse_top_k_categorical_accuracy: 0.8294\n",
      "Epoch 48/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9479 - sparse_categorical_accuracy: 0.2992 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9392 - val_sparse_categorical_accuracy: 0.2991 - val_sparse_top_k_categorical_accuracy: 0.8260\n",
      "Epoch 49/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9470 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8270 - val_loss: 1.9376 - val_sparse_categorical_accuracy: 0.3009 - val_sparse_top_k_categorical_accuracy: 0.8280\n",
      "Epoch 50/100\n",
      "102681/102681 [==============================] - 50s 488us/step - loss: 1.9475 - sparse_categorical_accuracy: 0.2980 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9342 - val_sparse_categorical_accuracy: 0.3031 - val_sparse_top_k_categorical_accuracy: 0.8280\n",
      "Epoch 51/100\n",
      "102681/102681 [==============================] - 50s 489us/step - loss: 1.9474 - sparse_categorical_accuracy: 0.2981 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9321 - val_sparse_categorical_accuracy: 0.2975 - val_sparse_top_k_categorical_accuracy: 0.8288\n",
      "Epoch 52/100\n",
      "102681/102681 [==============================] - 50s 491us/step - loss: 1.9472 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8253 - val_loss: 1.9348 - val_sparse_categorical_accuracy: 0.3047 - val_sparse_top_k_categorical_accuracy: 0.8276\n",
      "Epoch 53/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9478 - sparse_categorical_accuracy: 0.2974 - sparse_top_k_categorical_accuracy: 0.8250 - val_loss: 1.9293 - val_sparse_categorical_accuracy: 0.3030 - val_sparse_top_k_categorical_accuracy: 0.8274\n",
      "Epoch 54/100\n",
      "102681/102681 [==============================] - 50s 491us/step - loss: 1.9480 - sparse_categorical_accuracy: 0.2955 - sparse_top_k_categorical_accuracy: 0.8250 - val_loss: 1.9353 - val_sparse_categorical_accuracy: 0.2966 - val_sparse_top_k_categorical_accuracy: 0.8291\n",
      "Epoch 55/100\n",
      "102681/102681 [==============================] - 51s 492us/step - loss: 1.9482 - sparse_categorical_accuracy: 0.2958 - sparse_top_k_categorical_accuracy: 0.8253 - val_loss: 1.9402 - val_sparse_categorical_accuracy: 0.3004 - val_sparse_top_k_categorical_accuracy: 0.8284\n",
      "Epoch 56/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9475 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9362 - val_sparse_categorical_accuracy: 0.3006 - val_sparse_top_k_categorical_accuracy: 0.8288\n",
      "Epoch 57/100\n",
      "102681/102681 [==============================] - 53s 518us/step - loss: 1.9482 - sparse_categorical_accuracy: 0.2970 - sparse_top_k_categorical_accuracy: 0.8257 - val_loss: 1.9372 - val_sparse_categorical_accuracy: 0.3048 - val_sparse_top_k_categorical_accuracy: 0.8270\n",
      "Epoch 58/100\n",
      "102681/102681 [==============================] - 52s 509us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2976 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9382 - val_sparse_categorical_accuracy: 0.2996 - val_sparse_top_k_categorical_accuracy: 0.8283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "102681/102681 [==============================] - 53s 517us/step - loss: 1.9478 - sparse_categorical_accuracy: 0.2976 - sparse_top_k_categorical_accuracy: 0.8257 - val_loss: 1.9389 - val_sparse_categorical_accuracy: 0.3021 - val_sparse_top_k_categorical_accuracy: 0.8270\n",
      "Epoch 60/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9477 - sparse_categorical_accuracy: 0.2971 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9351 - val_sparse_categorical_accuracy: 0.3035 - val_sparse_top_k_categorical_accuracy: 0.8304\n",
      "Epoch 61/100\n",
      "102681/102681 [==============================] - 51s 496us/step - loss: 1.9469 - sparse_categorical_accuracy: 0.2975 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9355 - val_sparse_categorical_accuracy: 0.3008 - val_sparse_top_k_categorical_accuracy: 0.8271\n",
      "Epoch 62/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9475 - sparse_categorical_accuracy: 0.2968 - sparse_top_k_categorical_accuracy: 0.8260 - val_loss: 1.9359 - val_sparse_categorical_accuracy: 0.3024 - val_sparse_top_k_categorical_accuracy: 0.8266\n",
      "Epoch 63/100\n",
      "102681/102681 [==============================] - 51s 494us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2965 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9381 - val_sparse_categorical_accuracy: 0.3027 - val_sparse_top_k_categorical_accuracy: 0.8264\n",
      "Epoch 64/100\n",
      "102681/102681 [==============================] - 50s 483us/step - loss: 1.9472 - sparse_categorical_accuracy: 0.2986 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9378 - val_sparse_categorical_accuracy: 0.3035 - val_sparse_top_k_categorical_accuracy: 0.8247\n",
      "Epoch 65/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9470 - sparse_categorical_accuracy: 0.2981 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9366 - val_sparse_categorical_accuracy: 0.3046 - val_sparse_top_k_categorical_accuracy: 0.8262\n",
      "Epoch 66/100\n",
      "102681/102681 [==============================] - 50s 492us/step - loss: 1.9472 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8266 - val_loss: 1.9343 - val_sparse_categorical_accuracy: 0.3024 - val_sparse_top_k_categorical_accuracy: 0.8244\n",
      "Epoch 67/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9469 - sparse_categorical_accuracy: 0.2978 - sparse_top_k_categorical_accuracy: 0.8262 - val_loss: 1.9334 - val_sparse_categorical_accuracy: 0.3021 - val_sparse_top_k_categorical_accuracy: 0.8305\n",
      "Epoch 68/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2973 - sparse_top_k_categorical_accuracy: 0.8257 - val_loss: 1.9322 - val_sparse_categorical_accuracy: 0.3049 - val_sparse_top_k_categorical_accuracy: 0.8295\n",
      "Epoch 69/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9470 - sparse_categorical_accuracy: 0.2964 - sparse_top_k_categorical_accuracy: 0.8245 - val_loss: 1.9356 - val_sparse_categorical_accuracy: 0.3040 - val_sparse_top_k_categorical_accuracy: 0.8251\n",
      "Epoch 70/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9472 - sparse_categorical_accuracy: 0.2986 - sparse_top_k_categorical_accuracy: 0.8255 - val_loss: 1.9344 - val_sparse_categorical_accuracy: 0.3023 - val_sparse_top_k_categorical_accuracy: 0.8281\n",
      "Epoch 71/100\n",
      "102681/102681 [==============================] - 50s 490us/step - loss: 1.9474 - sparse_categorical_accuracy: 0.2969 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9347 - val_sparse_categorical_accuracy: 0.3029 - val_sparse_top_k_categorical_accuracy: 0.8283\n",
      "Epoch 72/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9467 - sparse_categorical_accuracy: 0.2976 - sparse_top_k_categorical_accuracy: 0.8267 - val_loss: 1.9357 - val_sparse_categorical_accuracy: 0.3024 - val_sparse_top_k_categorical_accuracy: 0.8281\n",
      "Epoch 73/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2986 - sparse_top_k_categorical_accuracy: 0.8255 - val_loss: 1.9296 - val_sparse_categorical_accuracy: 0.3037 - val_sparse_top_k_categorical_accuracy: 0.8316\n",
      "Epoch 74/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2978 - sparse_top_k_categorical_accuracy: 0.8260 - val_loss: 1.9401 - val_sparse_categorical_accuracy: 0.2997 - val_sparse_top_k_categorical_accuracy: 0.8278\n",
      "Epoch 75/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2989 - sparse_top_k_categorical_accuracy: 0.8252 - val_loss: 1.9311 - val_sparse_categorical_accuracy: 0.3026 - val_sparse_top_k_categorical_accuracy: 0.8279\n",
      "Epoch 76/100\n",
      "102681/102681 [==============================] - 50s 490us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2991 - sparse_top_k_categorical_accuracy: 0.8268 - val_loss: 1.9483 - val_sparse_categorical_accuracy: 0.2980 - val_sparse_top_k_categorical_accuracy: 0.8224\n",
      "Epoch 77/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2988 - sparse_top_k_categorical_accuracy: 0.8266 - val_loss: 1.9343 - val_sparse_categorical_accuracy: 0.3037 - val_sparse_top_k_categorical_accuracy: 0.8280\n",
      "Epoch 78/100\n",
      "102681/102681 [==============================] - 51s 493us/step - loss: 1.9465 - sparse_categorical_accuracy: 0.2981 - sparse_top_k_categorical_accuracy: 0.8246 - val_loss: 1.9338 - val_sparse_categorical_accuracy: 0.3037 - val_sparse_top_k_categorical_accuracy: 0.8282\n",
      "Epoch 79/100\n",
      "102681/102681 [==============================] - 50s 491us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9315 - val_sparse_categorical_accuracy: 0.3039 - val_sparse_top_k_categorical_accuracy: 0.8288\n",
      "Epoch 80/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9476 - sparse_categorical_accuracy: 0.2973 - sparse_top_k_categorical_accuracy: 0.8254 - val_loss: 1.9368 - val_sparse_categorical_accuracy: 0.3000 - val_sparse_top_k_categorical_accuracy: 0.8282\n",
      "Epoch 81/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9469 - sparse_categorical_accuracy: 0.2987 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9374 - val_sparse_categorical_accuracy: 0.3024 - val_sparse_top_k_categorical_accuracy: 0.8247\n",
      "Epoch 82/100\n",
      "102681/102681 [==============================] - 50s 491us/step - loss: 1.9463 - sparse_categorical_accuracy: 0.2971 - sparse_top_k_categorical_accuracy: 0.8250 - val_loss: 1.9371 - val_sparse_categorical_accuracy: 0.2990 - val_sparse_top_k_categorical_accuracy: 0.8283\n",
      "Epoch 83/100\n",
      "102681/102681 [==============================] - 54s 522us/step - loss: 1.9469 - sparse_categorical_accuracy: 0.2984 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9308 - val_sparse_categorical_accuracy: 0.3028 - val_sparse_top_k_categorical_accuracy: 0.8293\n",
      "Epoch 84/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9473 - sparse_categorical_accuracy: 0.2976 - sparse_top_k_categorical_accuracy: 0.8260 - val_loss: 1.9336 - val_sparse_categorical_accuracy: 0.3052 - val_sparse_top_k_categorical_accuracy: 0.8291\n",
      "Epoch 85/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2984 - sparse_top_k_categorical_accuracy: 0.8262 - val_loss: 1.9331 - val_sparse_categorical_accuracy: 0.3026 - val_sparse_top_k_categorical_accuracy: 0.8280\n",
      "Epoch 86/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2978 - sparse_top_k_categorical_accuracy: 0.8263 - val_loss: 1.9355 - val_sparse_categorical_accuracy: 0.2981 - val_sparse_top_k_categorical_accuracy: 0.8288\n",
      "Epoch 87/100\n",
      "102681/102681 [==============================] - 50s 487us/step - loss: 1.9469 - sparse_categorical_accuracy: 0.2964 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9346 - val_sparse_categorical_accuracy: 0.2980 - val_sparse_top_k_categorical_accuracy: 0.8278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "102681/102681 [==============================] - 50s 482us/step - loss: 1.9474 - sparse_categorical_accuracy: 0.2965 - sparse_top_k_categorical_accuracy: 0.8259 - val_loss: 1.9362 - val_sparse_categorical_accuracy: 0.3025 - val_sparse_top_k_categorical_accuracy: 0.8305\n",
      "Epoch 89/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9466 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8270 - val_loss: 1.9391 - val_sparse_categorical_accuracy: 0.3015 - val_sparse_top_k_categorical_accuracy: 0.8280\n",
      "Epoch 90/100\n",
      "102681/102681 [==============================] - 50s 486us/step - loss: 1.9469 - sparse_categorical_accuracy: 0.2977 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9354 - val_sparse_categorical_accuracy: 0.3016 - val_sparse_top_k_categorical_accuracy: 0.8279\n",
      "Epoch 91/100\n",
      "102681/102681 [==============================] - 49s 482us/step - loss: 1.9468 - sparse_categorical_accuracy: 0.2969 - sparse_top_k_categorical_accuracy: 0.8267 - val_loss: 1.9333 - val_sparse_categorical_accuracy: 0.3003 - val_sparse_top_k_categorical_accuracy: 0.8298\n",
      "Epoch 92/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9470 - sparse_categorical_accuracy: 0.2987 - sparse_top_k_categorical_accuracy: 0.8251 - val_loss: 1.9316 - val_sparse_categorical_accuracy: 0.3046 - val_sparse_top_k_categorical_accuracy: 0.8299\n",
      "Epoch 93/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9467 - sparse_categorical_accuracy: 0.2985 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9307 - val_sparse_categorical_accuracy: 0.3061 - val_sparse_top_k_categorical_accuracy: 0.8294\n",
      "Epoch 94/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9470 - sparse_categorical_accuracy: 0.2978 - sparse_top_k_categorical_accuracy: 0.8258 - val_loss: 1.9332 - val_sparse_categorical_accuracy: 0.3033 - val_sparse_top_k_categorical_accuracy: 0.8272\n",
      "Epoch 95/100\n",
      "102681/102681 [==============================] - 52s 502us/step - loss: 1.9471 - sparse_categorical_accuracy: 0.2974 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9325 - val_sparse_categorical_accuracy: 0.3040 - val_sparse_top_k_categorical_accuracy: 0.8263\n",
      "Epoch 96/100\n",
      "102681/102681 [==============================] - 49s 480us/step - loss: 1.9474 - sparse_categorical_accuracy: 0.2973 - sparse_top_k_categorical_accuracy: 0.8256 - val_loss: 1.9359 - val_sparse_categorical_accuracy: 0.3012 - val_sparse_top_k_categorical_accuracy: 0.8266\n",
      "Epoch 97/100\n",
      "102681/102681 [==============================] - 50s 485us/step - loss: 1.9472 - sparse_categorical_accuracy: 0.2988 - sparse_top_k_categorical_accuracy: 0.8251 - val_loss: 1.9390 - val_sparse_categorical_accuracy: 0.3022 - val_sparse_top_k_categorical_accuracy: 0.8274\n",
      "Epoch 98/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9464 - sparse_categorical_accuracy: 0.2992 - sparse_top_k_categorical_accuracy: 0.8262 - val_loss: 1.9319 - val_sparse_categorical_accuracy: 0.3033 - val_sparse_top_k_categorical_accuracy: 0.8288\n",
      "Epoch 99/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9472 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.8257 - val_loss: 1.9420 - val_sparse_categorical_accuracy: 0.3028 - val_sparse_top_k_categorical_accuracy: 0.8246\n",
      "Epoch 100/100\n",
      "102681/102681 [==============================] - 50s 484us/step - loss: 1.9477 - sparse_categorical_accuracy: 0.2982 - sparse_top_k_categorical_accuracy: 0.8261 - val_loss: 1.9363 - val_sparse_categorical_accuracy: 0.3035 - val_sparse_top_k_categorical_accuracy: 0.8273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5f374c88>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, class_weight=class_weights, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T02:19:32.923444Z",
     "start_time": "2019-12-09T02:19:27.785420Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"nn-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T02:19:36.461575Z",
     "start_time": "2019-12-09T02:19:32.926291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32088/32088 [==============================] - 3s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9397381964956724, 0.3001745200623779, 0.8258227374570918]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T02:19:59.499113Z",
     "start_time": "2019-12-09T02:19:36.464801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The fitted model:\n",
      "\n",
      "<keras.engine.sequential.Sequential object at 0x1c7196d978>\n",
      "\n",
      "\n",
      "This model resulted in the following accuracy:\n",
      "\n",
      "Training\t0.3031\n",
      "Test\t\t0.3002\n",
      "\n",
      "\n",
      "The model AUC is:\n",
      "\n",
      "\t\tweighted\tunweighted\n",
      "Training\t0.6294\t\t0.6234\n",
      "Test\t\t0.6244\t\t0.6169\n",
      "\n",
      "\n",
      "The number of classes predicted by class are:\n",
      "\n",
      "TRAINING\n",
      "       count  proportion\n",
      "class                   \n",
      "0       1050    0.008181\n",
      "1         31    0.000242\n",
      "2       4465    0.034787\n",
      "4      42490    0.331043\n",
      "6      77128    0.600910\n",
      "7          1    0.000008\n",
      "8       3187    0.024830\n",
      "\n",
      "TEST\n",
      "       count  proportion\n",
      "class                   \n",
      "0        223    0.006950\n",
      "1          8    0.000249\n",
      "2       1143    0.035621\n",
      "4      10681    0.332866\n",
      "6      19237    0.599508\n",
      "8        796    0.024807\n",
      "\n",
      "\n",
      "The resulting confusion matrix:\n",
      "\n",
      "TRAINING\n",
      "Actual        0     1      2     3      4     5      6      7      8   Total\n",
      "Predicted                                                                   \n",
      "0           326     8    252    41     94    23    126     57    123    1050\n",
      "1             0     3      2     9      3     1     11      1      1      31\n",
      "2           225   134   1825   243    298   106    818    282    534    4465\n",
      "3             0     0      0     0      0     0      0      0      0       0\n",
      "4          1681  1841   3370  2786  10652  1205   7774   5205   7976   42490\n",
      "5             0     0      0     0      0     0      0      0      0       0\n",
      "6          3849  3562   7419  6334   9259  1933  25235   7790  11747   77128\n",
      "7             0     0      0     0      1     0      0      0      0       1\n",
      "8           240   116    214   174    460   155    591    375    862    3187\n",
      "Total      6321  5664  13082  9587  20767  3423  34555  13710  21243  128352\n",
      "\n",
      "TEST\n",
      "Actual        0     1     2     3     4    5     6     7     8  Total\n",
      "Predicted                                                            \n",
      "0            58     0    51     9    25    9    36    11    24    223\n",
      "1             1     0     0     4     0    0     3     0     0      8\n",
      "2            64    44   452    60    74   30   199    80   140   1143\n",
      "3             0     0     0     0     0    0     0     0     0      0\n",
      "4           446   457   848   683  2702  284  2015  1264  1982  10681\n",
      "5             0     0     0     0     0    0     0     0     0      0\n",
      "6           961   882  1858  1605  2263  498  6227  1972  2971  19237\n",
      "7             0     0     0     0     0    0     0     0     0      0\n",
      "8            50    33    62    36   128   35   159   100   193    796\n",
      "Total      1580  1416  3271  2397  5192  856  8639  3427  5310  32088\n",
      "\n",
      "\n",
      "The classification metrics derived from the confusion matrix are:\n",
      "\n",
      "TRAINING\n",
      "          TP     FP     FN      TN       TPR       FNR       FPR       TNR\n",
      "class                                                                     \n",
      "0        326    724   5995  121307  0.051574  0.948426  0.005933  0.994067\n",
      "1          3     28   5661  122660  0.000530  0.999470  0.000228  0.999772\n",
      "2       1825   2640  11257  112630  0.139505  0.860495  0.022903  0.977097\n",
      "3          0      0   9587  118765  0.000000  1.000000  0.000000  1.000000\n",
      "4      10652  31838  10115   75747  0.512929  0.487071  0.295933  0.704067\n",
      "5          0      0   3423  124929  0.000000  1.000000  0.000000  1.000000\n",
      "6      25235  51893   9320   41904  0.730285  0.269715  0.553248  0.446752\n",
      "7          0      1  13710  114641  0.000000  1.000000  0.000009  0.999991\n",
      "8        862   2325  20381  104784  0.040578  0.959422  0.021707  0.978293\n",
      "\n",
      "TEST\n",
      "         TP     FP    FN     TN       TPR       FNR       FPR       TNR\n",
      "class                                                                  \n",
      "0        58    165  1522  30343  0.036709  0.963291  0.005408  0.994592\n",
      "1         0      8  1416  30664  0.000000  1.000000  0.000261  0.999739\n",
      "2       452    691  2819  28126  0.138184  0.861816  0.023979  0.976021\n",
      "3         0      0  2397  29691  0.000000  1.000000  0.000000  1.000000\n",
      "4      2702   7979  2490  18917  0.520416  0.479584  0.296661  0.703339\n",
      "5         0      0   856  31232  0.000000  1.000000  0.000000  1.000000\n",
      "6      6227  13010  2412  10439  0.720801  0.279199  0.554821  0.445179\n",
      "7         0      0  3427  28661  0.000000  1.000000  0.000000  1.000000\n",
      "8       193    603  5117  26175  0.036347  0.963653  0.022518  0.977482\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_dict = generate_model_dict(\n",
    "    model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    class_dict,\n",
    "    random_state=20,\n",
    "    fitm=True #do not re-fit\n",
    ")\n",
    "\n",
    "print_model_results(nn_dict,accuracy='both', auc='both',\n",
    "                        pred_counts='both', conf_matrix='both',\n",
    "                        class_metrics='both')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
